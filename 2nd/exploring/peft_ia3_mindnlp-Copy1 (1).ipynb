{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e865045",
   "metadata": {},
   "source": [
    "IA3配条代码参考自：\n",
    "\n",
    "[peft_ia3_mindnlp.ipynb](https://github.com/mindspore-lab/mindnlp/tree/master/llm/peft/ia3/seq_2_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1abbd02",
   "metadata": {},
   "source": [
    "模型配置参考：基于MindSpore NLP的Roberta模型Prompt Tuning\n",
    "\n",
    "https://blog.csdn.net/Kenji_Shinji/article/details/144395136"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f93b7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.309 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mindspore\n",
    "from mindnlp.transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from mindnlp.peft import get_peft_config, get_peft_model, get_peft_model_state_dict, LoraConfig, TaskType,IA3Config\n",
    "from mindnlp.dataset import load_dataset\n",
    "from mindnlp.core import ops\n",
    "from mindnlp.common.optimization import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from mindnlp import evaluate\n",
    "\n",
    "model_name_or_path = \"roberta-large\"\n",
    "tokenizer_name_or_path = \"roberta-large\"\n",
    "\n",
    "# checkpoint_name = \"financial_sentiment_analysis_lora_v1.ckpt\"\n",
    "checkpoint_name = \"RoBERTa_IA3_v1.ckpt\"\n",
    "max_length = 128\n",
    "lr = 1e-3\n",
    "num_epochs = 2\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d0850ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MS_ALLOC_CONF]Runtime config:  enable_vmm:True  vmm_align_size:2MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,223,682 || all params: 356,585,476 || trainable%: 0.34316652874555104\n"
     ]
    }
   ],
   "source": [
    "# creating model\n",
    "peft_config = IA3Config(task_type=TaskType.SEQ_CLS, inference_mode=False) #TaskType从SEQ_2_SEQ_LM修改为SEQ_CLS\n",
    "#TaskType.SEQ_2_SEQ_LM 是专门为序列到序列生成任务（如机器翻译、文本摘要等）设计的。对于 RoBERTa 模型和 AutoModelForSequenceClassification，你应该将 task_type 设置为 TaskType.SEQ_CLS，因为这是针对序列分类任务的正确配置。\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path) #对应修改为AutoModelForSequenceClassification\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0f9579f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindnlp/transformers/tokenization_utils_base.py:1526: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted, and will be then set to `False` by default. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#加载tokenizer\n",
    "if any(k in model_name_or_path for k in (\"gpt\", \"opt\", \"bloom\")):\n",
    "    padding_side = \"left\"\n",
    "else:\n",
    "    padding_side = \"right\"\n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side=padding_side)\n",
    "if getattr(tokenizer, \"pad_token_id\") is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ee2babf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence1': Tensor(shape=[], dtype=String, value= 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .'), 'sentence2': Tensor(shape=[], dtype=String, value= 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .'), 'label': Tensor(shape=[], dtype=Int64, value= 1), 'idx': Tensor(shape=[], dtype=Int64, value= 0)}\n"
     ]
    }
   ],
   "source": [
    "mindspore.dataset.config.set_seed(123)\n",
    "# loading dataset\n",
    "# dataset = load_dataset(\"financial_phrasebank\", \"sentences_allagree\")\n",
    "datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "print(next(datasets['train'].create_dict_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f226e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp.dataset import BaseMapFunction\n",
    " \n",
    "class MapFunc(BaseMapFunction):\n",
    "    def __call__(self, sentence1, sentence2, label, idx):\n",
    "        outputs = tokenizer(sentence1, sentence2, truncation=True, max_length=None)\n",
    "        return outputs['input_ids'], outputs['attention_mask'], label\n",
    " \n",
    "def get_dataset(dataset, tokenizer):\n",
    "    input_colums=['sentence1', 'sentence2', 'label', 'idx']\n",
    "    output_columns=['input_ids', 'attention_mask', 'labels']\n",
    "    dataset = dataset.map(MapFunc(input_colums, output_columns),\n",
    "                          input_colums, output_columns)\n",
    "    dataset = dataset.padded_batch(batch_size, pad_info={'input_ids': (None, tokenizer.pad_token_id),\n",
    "                                                         'attention_mask': (None, 0)})\n",
    "    return dataset\n",
    " \n",
    "train_dataset = get_dataset(datasets['train'], tokenizer)\n",
    "eval_dataset = get_dataset(datasets['validation'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e052eea-f6d6-4a22-a71a-bef47abafffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"glue\", \"mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ceb546f-8bf9-4ded-ab8d-cab5048594ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,223,682 || all params: 356,585,476 || trainable%: 0.34316652874555104\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, return_dict=True)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f733a3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp.core import optim\n",
    "# optimizer and lr scheduler\n",
    "optimizer = optim.AdamW(model.trainable_params(), lr=lr)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataset) * num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b3a4090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/459 [00:12<1:35:35, 12.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 459/459 [03:35<00:00,  2.13it/s]\n",
      "100%|██████████| 51/51 [00:06<00:00,  7.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0: train_ppl=Tensor(shape=[], dtype=Float32, value= 2.04565) train_epoch_loss=Tensor(shape=[], dtype=Float32, value= 0.715715) eval_ppl=Tensor(shape=[], dtype=Float32, value= 1.95987) eval_epoch_loss=Tensor(shape=[], dtype=Float32, value= 0.672879)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 459/459 [02:48<00:00,  2.72it/s]\n",
      "100%|██████████| 51/51 [00:06<00:00,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1: train_ppl=Tensor(shape=[], dtype=Float32, value= 1.98013) train_epoch_loss=Tensor(shape=[], dtype=Float32, value= 0.683162) eval_ppl=Tensor(shape=[], dtype=Float32, value= 1.99289) eval_epoch_loss=Tensor(shape=[], dtype=Float32, value= 0.689586)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 459/459 [02:55<00:00,  2.62it/s]\n",
      "100%|██████████| 51/51 [00:06<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2: train_ppl=Tensor(shape=[], dtype=Float32, value= 1.96572) train_epoch_loss=Tensor(shape=[], dtype=Float32, value= 0.675861) eval_ppl=Tensor(shape=[], dtype=Float32, value= 2.08507) eval_epoch_loss=Tensor(shape=[], dtype=Float32, value= 0.734802)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 459/459 [02:51<00:00,  2.68it/s]\n",
      "100%|██████████| 51/51 [00:05<00:00,  9.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3: train_ppl=Tensor(shape=[], dtype=Float32, value= 1.94627) train_epoch_loss=Tensor(shape=[], dtype=Float32, value= 0.665914) eval_ppl=Tensor(shape=[], dtype=Float32, value= 2.0397) eval_epoch_loss=Tensor(shape=[], dtype=Float32, value= 0.712801)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 459/459 [02:31<00:00,  3.02it/s]\n",
      "100%|██████████| 51/51 [00:05<00:00,  9.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4: train_ppl=Tensor(shape=[], dtype=Float32, value= 1.93121) train_epoch_loss=Tensor(shape=[], dtype=Float32, value= 0.658147) eval_ppl=Tensor(shape=[], dtype=Float32, value= 1.99712) eval_epoch_loss=Tensor(shape=[], dtype=Float32, value= 0.691704)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 459/459 [02:34<00:00,  2.96it/s]\n",
      "100%|██████████| 51/51 [00:06<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5: train_ppl=Tensor(shape=[], dtype=Float32, value= 1.91593) train_epoch_loss=Tensor(shape=[], dtype=Float32, value= 0.650202) eval_ppl=Tensor(shape=[], dtype=Float32, value= 1.89734) eval_epoch_loss=Tensor(shape=[], dtype=Float32, value= 0.640451)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 459/459 [03:05<00:00,  2.47it/s]\n",
      "100%|██████████| 51/51 [00:06<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=6: train_ppl=Tensor(shape=[], dtype=Float32, value= 1.91161) train_epoch_loss=Tensor(shape=[], dtype=Float32, value= 0.647945) eval_ppl=Tensor(shape=[], dtype=Float32, value= 1.86912) eval_epoch_loss=Tensor(shape=[], dtype=Float32, value= 0.625467)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 459/459 [03:03<00:00,  2.49it/s]\n",
      "100%|██████████| 51/51 [00:05<00:00,  9.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=7: train_ppl=Tensor(shape=[], dtype=Float32, value= 1.90967) train_epoch_loss=Tensor(shape=[], dtype=Float32, value= 0.646929) eval_ppl=Tensor(shape=[], dtype=Float32, value= 1.86869) eval_epoch_loss=Tensor(shape=[], dtype=Float32, value= 0.625236)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 459/459 [02:51<00:00,  2.67it/s]\n",
      "100%|██████████| 51/51 [00:06<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=8: train_ppl=Tensor(shape=[], dtype=Float32, value= 1.89478) train_epoch_loss=Tensor(shape=[], dtype=Float32, value= 0.639105) eval_ppl=Tensor(shape=[], dtype=Float32, value= 1.8693) eval_epoch_loss=Tensor(shape=[], dtype=Float32, value= 0.625565)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 459/459 [02:45<00:00,  2.77it/s]\n",
      "100%|██████████| 51/51 [00:05<00:00,  9.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=9: train_ppl=Tensor(shape=[], dtype=Float32, value= 1.88924) train_epoch_loss=Tensor(shape=[], dtype=Float32, value= 0.636177) eval_ppl=Tensor(shape=[], dtype=Float32, value= 1.86806) eval_epoch_loss=Tensor(shape=[], dtype=Float32, value= 0.624898)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from mindnlp.core import value_and_grad\n",
    "# 导入value_and_grad用于计算损失值和梯度\n",
    "# training and evaluation\n",
    "# 定义前向传播函数\n",
    "def forward_fn(**batch):\n",
    "     #\"\"\"前向传播函数，用于计算模型的损失值。\n",
    "    \n",
    "    #参数:\n",
    "    #**batch: 一个批次的数据，具体结构取决于模型的输入要求。\n",
    "    \n",
    "    #返回:\n",
    "    #loss: 模型的损失值。\"\"\"\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "    return loss\n",
    "\n",
    "# 使用value_and_grad装饰前向传播函数，以便在训练时获取损失值和梯度\n",
    "grad_fn = value_and_grad(forward_fn, model.trainable_params())\n",
    "\n",
    "# 开始训练和评估循环\n",
    "for epoch in range(num_epochs):\n",
    "    # 设置模型为训练模式\n",
    "    model.set_train()\n",
    "    total_loss = 0\n",
    "    train_total_size = train_dataset.get_dataset_size()\n",
    "    # 遍历训练数据集\n",
    "    for step, batch in enumerate(tqdm(train_dataset.create_dict_iterator(), total=train_total_size)):\n",
    "        # 清除上一步的梯度\n",
    "        optimizer.zero_grad()\n",
    "        # 计算损失值和梯度\n",
    "        loss = grad_fn(**batch)\n",
    "         # 更新模型参数\n",
    "        optimizer.step()\n",
    "        # 累加损失值\n",
    "        total_loss += loss.float()\n",
    "        # 更新学习率调度器\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    # 设置模型为评估模式\n",
    "    model.set_train(False)\n",
    "    eval_loss = 0\n",
    "    eval_preds = []\n",
    "    eval_total_size = eval_dataset.get_dataset_size()\n",
    "    # 遍历评估数据集\n",
    "    for step, batch in enumerate(tqdm(eval_dataset.create_dict_iterator(), total=eval_total_size)):\n",
    "        # 禁用梯度计算，进行前向传播\n",
    "        with mindspore._no_grad():\n",
    "            outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.float()\n",
    "         # 将模型预测结果解码为文本并存储\n",
    "       #  eval_preds.extend(\n",
    "       #     tokenizer.batch_decode(ops.argmax(outputs.logits, -1).asnumpy(), skip_special_tokens=True)\n",
    "       # )\n",
    "       # 对于序列分类任务，直接获取类别索引\n",
    "       eval_preds.extend(ops.argmax(outputs.logits, -1).asnumpy().tolist())\n",
    "\n",
    "# 计算评估期损失\n",
    "    eval_epoch_loss = eval_loss / len(eval_dataset)\n",
    "    eval_ppl = ops.exp(eval_epoch_loss)\n",
    "# 计算训练期损失\n",
    "    train_epoch_loss = total_loss / len(train_dataset)\n",
    "    train_ppl = ops.exp(train_epoch_loss)\n",
    "# 打印训练和评估结果\n",
    "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b95a29e-3b1c-493e-916d-cb7bd5c1f44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_preds=['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "print(f\"{eval_preds=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b6c252c-f6d1-41f0-935b-e7b2aed0863f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=0.0 % on the evaluation dataset\n",
      "eval_preds[:10]=['', '', '', '', '', '', '', '', '', '']\n",
      "ground_truth[:10]=['1', '0', '0', '1', '0', '1', '0', '1', '1', '1']\n"
     ]
    }
   ],
   "source": [
    "# print accuracy\n",
    "# 初始化正确预测和总样本的计数器\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# 初始化用于存储真实标签的列表\n",
    "ground_truth = []\n",
    "\n",
    "# 遍历预测结果和验证数据集\n",
    "for pred, data in zip(eval_preds, datasets['validation'].create_dict_iterator(output_numpy=True)):\n",
    "    # 获取真实的文本标签\n",
    "    true = str(data['label'])\n",
    "    # 将真实标签添加到ground_truth列表中\n",
    "    ground_truth.append(true)\n",
    "    # 如果预测的标签与真实标签一致，则正确计数器加一\n",
    "    if pred.strip() == true.strip():\n",
    "        correct += 1\n",
    "    # 总样本计数器加一\n",
    "    total += 1\n",
    "# 计算准确率\n",
    "accuracy = correct / total * 100\n",
    "# 输出准确率\n",
    "print(f\"{accuracy=} % on the evaluation dataset\")\n",
    "# 输出前10个预测结果\n",
    "print(f\"{eval_preds[:10]=}\")\n",
    "# 输出前10个真实标签\n",
    "print(f\"{ground_truth[:10]=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77adfbb0-f8fc-43e0-aad2-42d7f604cbf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <mindspore.dataset.engine.datasets_user_defined.GeneratorDataset at 0xfffe38bb9a60>,\n",
       " 'validation': <mindspore.dataset.engine.datasets_user_defined.GeneratorDataset at 0xfffdfc0f5370>,\n",
       " 'test': <mindspore.dataset.engine.datasets_user_defined.GeneratorDataset at 0xffff610c9df0>}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26e88eb1-3d24-488b-ad62-0c18b81b861e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tensor(shape=[8, 73], dtype=Int64, value=\n",
       " [[    0,   894,    26 ...     1,     1,     1],\n",
       "  [    0, 43600,  1322 ...     1,     1,     1],\n",
       "  [    0,   133,  1404 ...   135,   479,     2],\n",
       "  ...\n",
       "  [    0, 30888,    12 ...     1,     1,     1],\n",
       "  [    0,  5771,   385 ...     1,     1,     1],\n",
       "  [    0,   713, 33752 ...     1,     1,     1]]),\n",
       " Tensor(shape=[8, 73], dtype=Int64, value=\n",
       " [[1, 1, 1 ... 0, 0, 0],\n",
       "  [1, 1, 1 ... 0, 0, 0],\n",
       "  [1, 1, 1 ... 1, 1, 1],\n",
       "  ...\n",
       "  [1, 1, 1 ... 0, 0, 0],\n",
       "  [1, 1, 1 ... 0, 0, 0],\n",
       "  [1, 1, 1 ... 0, 0, 0]]),\n",
       " Tensor(shape=[8], dtype=Int64, value= [1, 0, 0, 1, 0, 1, 0, 1])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取数据集迭代器的下一个元素\n",
    "next(eval_dataset.create_tuple_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8de6005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model\n",
    "# 构建PEFT模型的唯一标识符\n",
    "# 该标识符结合了模型名称或路径、PEFT配置的类型和任务类型，用于区分不同的PEFT模型\n",
    "peft_model_id = f\"{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\"\n",
    "\n",
    "# 保存预训练的PEFT模型\n",
    "# 使用构建的PEFT模型标识符作为模型ID，将模型保存到指定目录或路径\n",
    "model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd20cd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.7M\troberta-large_IA3_SEQ_CLS/adapter_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# 构建checkpoint文件的路径\n",
    "ckpt = f\"{peft_model_id}/adapter_model.ckpt\"\n",
    "# 使用shell命令检查checkpoint文件的大小\n",
    "!du -h $ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76c2fc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 导入Peft模型和配置相关的模块\n",
    "from mindnlp.peft import PeftModel, PeftConfig\n",
    "\n",
    "# 构造Peft模型的唯一标识符\n",
    "# 这里使用模型名称或路径、Peft配置的类型和任务类型来组合成一个字符串\n",
    "peft_model_id = f\"{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\"\n",
    "\n",
    "# 从预训练的Peft模型中加载配置\n",
    "# 这里的配置将指导如何加载和使用Peft模型\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "# 从配置中加载基础模型\n",
    "# 这个基础模型是用于执行序列到序列语言模型任务的\n",
    "model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path)\n",
    "# 使用Peft模型对基础模型进行微调\n",
    "# 这一步是将基础模型与Peft模型的特定微调参数结合起来\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37d712ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validation_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mset_train(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 从验证数据集中获取一个样本，以便后续进行推理演示\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[43mvalidation_dataset\u001b[49m\u001b[38;5;241m.\u001b[39mcreate_dict_iterator(output_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 打印样本中的文本标签，以便用户了解正在处理的数据\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124midx\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'validation_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# 设置模型为评估模式，以禁用dropout等训练时的行为\n",
    "model.set_train(False)\n",
    "# 从验证数据集中获取一个样本，以便后续进行推理演示\n",
    "example = next(validation_dataset.create_dict_iterator(output_numpy=True))\n",
    "\n",
    "# 打印样本中的文本标签，以便用户了解正在处理的数据\n",
    "print(example['idx'])\n",
    "# 使用tokenizer对样本文本进行编码，以将其转换为模型可处理的输入格式\n",
    "inputs = tokenizer(example['idx'], return_tensors=\"ms\")\n",
    "# 打印编码后的输入，以展示输入格式和内容\n",
    "print(inputs)\n",
    "\n",
    "# 禁用梯度计算，以减少计算资源消耗，因为推理过程不需要反向传播\n",
    "with mindspore._no_grad():\n",
    "    # 使用模型生成文本，指定最大新生成的令牌数量以限制输出长度\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10)\n",
    "    # 打印生成的文本编码，以展示模型的输出内容\n",
    "    print(outputs)\n",
    "    # 将生成的文本编码转换回人类可读的文本格式，并打印\n",
    "    print(tokenizer.batch_decode(outputs.asnumpy(), skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
