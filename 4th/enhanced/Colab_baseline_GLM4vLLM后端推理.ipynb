{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "18a8de8f",
      "metadata": {
        "id": "18a8de8f"
      },
      "source": [
        "# 基础思路（baseline）"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5861209f",
      "metadata": {
        "id": "5861209f"
      },
      "source": [
        "因为transformer后端推理十分缓慢，导致了对计算单元（核时）的消耗非常大。尝试使用vLLM对GLM4-9B-CHAT模型进行后端推理，以减少对计算单元的消耗。\n",
        "\n",
        "vLLM的推理是增加吞吐量的方式，对于显存的使用策略非常激进，会占据大量的显存空间，请保证至少36GB以上的显存。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa932cfc",
      "metadata": {
        "id": "fa932cfc"
      },
      "source": [
        "## 步骤1：更新或安装所需环境"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6724776c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6724776c",
        "outputId": "0ece773e-ca50-48f1-ecdc-1a8c607b25b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (2.2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Collecting tiktoken\n",
            "  Using cached tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: vllm in /usr/local/lib/python3.10/dist-packages (0.6.3.post1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from vllm) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from vllm) (0.2.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from vllm) (9.0.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from vllm) (3.20.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from vllm) (3.10.10)\n",
            "Requirement already satisfied: openai>=1.40.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (1.52.2)\n",
            "Requirement already satisfied: uvicorn[standard] in /usr/local/lib/python3.10/dist-packages (from vllm) (0.32.0)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.10/dist-packages (from vllm) (2.9.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from vllm) (10.4.0)\n",
            "Requirement already satisfied: prometheus-client>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.21.0)\n",
            "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (7.0.0)\n",
            "Requirement already satisfied: lm-format-enforcer==0.10.6 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.10.6)\n",
            "Requirement already satisfied: outlines<0.1,>=0.0.43 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.0.46)\n",
            "Requirement already satisfied: typing-extensions>=4.10 in /usr/local/lib/python3.10/dist-packages (from vllm) (4.12.2)\n",
            "Requirement already satisfied: partial-json-parser in /usr/local/lib/python3.10/dist-packages (from vllm) (0.2.1.1.post4)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.10/dist-packages (from vllm) (24.0.1)\n",
            "Requirement already satisfied: msgspec in /usr/local/lib/python3.10/dist-packages (from vllm) (0.18.6)\n",
            "Requirement already satisfied: gguf==0.10.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.10.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from vllm) (8.5.0)\n",
            "Requirement already satisfied: mistral-common>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from mistral-common[opencv]>=1.4.4->vllm) (1.4.4)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from vllm) (0.8.0)\n",
            "Requirement already satisfied: compressed-tensors==0.6.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.6.0)\n",
            "Requirement already satisfied: ray>=2.9 in /usr/local/lib/python3.10/dist-packages (from vllm) (2.38.0)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.10/dist-packages (from vllm) (12.560.30)\n",
            "Requirement already satisfied: torch==2.4.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (2.4.0)\n",
            "Requirement already satisfied: torchvision==0.19 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.19.0)\n",
            "Requirement already satisfied: xformers==0.0.27.post2 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.0.27.post2)\n",
            "Requirement already satisfied: fastapi!=0.113.*,!=0.114.0,>=0.107.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.115.4)\n",
            "Requirement already satisfied: interegular>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from lm-format-enforcer==0.10.6->vllm) (0.3.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->vllm) (12.6.77)\n",
            "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm) (0.41.2)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.21.1 in /usr/local/lib/python3.10/dist-packages (from mistral-common>=1.4.4->mistral-common[opencv]>=1.4.4->vllm) (4.23.0)\n",
            "Requirement already satisfied: opencv-python-headless<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mistral-common[opencv]>=1.4.4->vllm) (4.10.0.84)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->vllm) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.40.0->vllm) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->vllm) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->vllm) (0.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->vllm) (1.3.1)\n",
            "Requirement already satisfied: lark in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm) (1.2.2)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm) (1.6.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm) (3.1.0)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm) (5.6.3)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm) (0.60.0)\n",
            "Requirement already satisfied: referencing in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm) (0.35.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm) (3.0.2)\n",
            "Requirement already satisfied: pycountry in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm) (24.6.1)\n",
            "Requirement already satisfied: pyairports in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm) (2.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9->vllm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9->vllm) (2.23.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (8.1.7)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (1.1.0)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (1.4.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (2.4.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (24.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (4.0.3)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->vllm) (3.20.2)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm) (0.24.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm) (13.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.40.0->vllm) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.40.0->vllm) (1.0.6)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common>=1.4.4->mistral-common[opencv]>=1.4.4->vllm) (2024.10.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common>=1.4.4->mistral-common[opencv]>=1.4.4->vllm) (0.20.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->vllm) (0.2.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm) (0.70.16)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0->vllm) (3.0.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->outlines<0.1,>=0.0.43->vllm) (0.43.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0->vllm) (1.3.0)\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers requests urllib3 tqdm pandas tiktoken vllm\n",
        "!apt update > /dev/null; apt install aria2 git-lfs axel -y > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d22db0c3",
      "metadata": {
        "id": "d22db0c3"
      },
      "source": [
        "增加了 transformers 、tiktoken、vllm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "683b4361",
      "metadata": {
        "id": "683b4361"
      },
      "source": [
        "## 步骤2：下载数据集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5e631c84",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-22T05:26:17.398404Z",
          "iopub.status.busy": "2024-10-22T05:26:17.398143Z",
          "iopub.status.idle": "2024-10-22T05:26:17.773504Z",
          "shell.execute_reply": "2024-10-22T05:26:17.772988Z",
          "shell.execute_reply.started": "2024-10-22T05:26:17.398385Z"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e631c84",
        "outputId": "e5bb3ce8-aa78-4786-a1e4-80f340195a0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing download: https://ai-contest-static.xfyun.cn/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%83%BD%E5%8A%9B%E8%AF%84%E6%B5%8B%EF%BC%9A%E4%B8%AD%E6%96%87%E6%88%90%E8%AF%AD%E9%87%8A%E4%B9%89%E4%B8%8E%E8%A7%A3%E6%9E%90%E6%8C%91%E6%88%98%E8%B5%9B/test_input.csv\n",
            "File size: 131.332 Kilobyte(s) (134484 bytes)\n",
            "Opening output file test_input.csv.2\n",
            "Starting download\n",
            "\n",
            "Can't setup alternate output. Deactivating.\n",
            "..... .......... .......... .......... ..........  [ 259.8KB/s]\n",
            "[ 38%]  .......... .......... .......... .......... ..........  [ 467.3KB/s]\n",
            "[ 76%]  .......... .......... .......... .\n",
            "\n",
            "Downloaded 131.332 Kilobyte(s) in 0 second(s). (547.49 KB/s)\n"
          ]
        }
      ],
      "source": [
        "!axel -n 12 -a https://ai-contest-static.xfyun.cn/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%83%BD%E5%8A%9B%E8%AF%84%E6%B5%8B%EF%BC%9A%E4%B8%AD%E6%96%87%E6%88%90%E8%AF%AD%E9%87%8A%E4%B9%89%E4%B8%8E%E8%A7%A3%E6%9E%90%E6%8C%91%E6%88%98%E8%B5%9B/test_input.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e13b85fa",
      "metadata": {
        "id": "e13b85fa"
      },
      "source": [
        "## 步骤3：构建模型（使用GLM-4-9B-Chat）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5c0e04dd",
      "metadata": {
        "ExecutionIndicator": {
          "show": true
        },
        "execution": {
          "iopub.execute_input": "2024-10-22T05:26:17.774571Z",
          "iopub.status.busy": "2024-10-22T05:26:17.774318Z"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381,
          "referenced_widgets": [
            "891607d15775446ca8f6c518bf335673",
            "723a52242bac430b97a607b186dddc74",
            "ce19752078d6444dbe80f73045477346",
            "e83363cb2184413590238f99f62667d3",
            "075ae5356a1d4358a0b86205b15d4f49",
            "f3dd29182748439ea862b822b6d81a49",
            "1b3b1720befe41fa90d423d7dd4c5731",
            "cad687a7b5084c8ca7e2300090e93e7c",
            "2465db233c024598a621a68e0e986222",
            "34393296c08c40e2b0eb3c7888af9136",
            "add46b1cf2724f7e9aec0ffe280f72a8"
          ]
        },
        "id": "5c0e04dd",
        "outputId": "05224ca6-90cd-4b09-ec12-a6d9141b26a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 10-29 03:09:00 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
            "INFO 10-29 03:09:00 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='THUDM/glm-4-9b-chat', speculative_config=None, tokenizer='THUDM/glm-4-9b-chat', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=THUDM/glm-4-9b-chat, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n",
            "WARNING 10-29 03:09:01 tokenizer.py:169] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
            "INFO 10-29 03:09:01 model_runner.py:1056] Starting to load model THUDM/glm-4-9b-chat...\n",
            "INFO 10-29 03:09:02 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/10 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "891607d15775446ca8f6c518bf335673"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 10-29 03:09:08 model_runner.py:1067] Loading model weights took 17.5635 GB\n",
            "INFO 10-29 03:09:09 gpu_executor.py:122] # GPU blocks: 26291, # CPU blocks: 6553\n",
            "INFO 10-29 03:09:09 gpu_executor.py:126] Maximum concurrency for 1024 tokens per request: 410.80x\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s, est. speed input: 42.67 toks/s, output: 28.44 toks/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RequestOutput(request_id=0, prompt='[gMASK]<sop><|user|>\\n列举与下面句子最符合的五个成语。只需要输出五个成语，不需要有其他的输出，写在一行中：比喻夫妻关系和谐<|assistant|>', prompt_token_ids=[151331, 151333, 151331, 151333, 151336, 198, 115571, 98381, 101534, 108259, 98430, 100498, 98314, 105053, 109924, 1773, 107073, 102162, 105053, 109924, 3837, 103628, 98318, 106911, 102162, 3837, 99032, 104120, 98351, 98322, 5122, 110573, 102885, 99172, 101938, 151337], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='\\n相敬如宾、琴瑟和鸣、恩爱夫妻、伉俪情深、夫唱妇随', token_ids=(198, 98463, 100619, 98410, 100907, 5373, 101396, 104899, 98327, 102033, 5373, 118219, 102885, 5373, 17406, 231, 123143, 125894, 5373, 99324, 99918, 99991, 98932, 151336), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=151336)], finished=True, metrics=RequestMetrics(arrival_time=1730171352.6092703, last_token_time=1730171352.6092703, first_scheduled_time=1730171352.6124427, first_token_time=1730171352.6596408, time_in_queue=0.0031723976135253906, finished_time=1730171353.4557714, scheduler_time=0.0033692079998672853, model_forward_time=None, model_execute_time=None), lora_request=None)\n",
            "\n",
            "相敬如宾、琴瑟和鸣、恩爱夫妻、伉俪情深、夫唱妇随\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from vllm import LLM, SamplingParams\n",
        "import torch\n",
        "\n",
        "# GLM-4-9B-Chat-1M\n",
        "# max_model_len, tp_size = 1048576, 4\n",
        "\n",
        "# GLM-4-9B-Chat\n",
        "# 如果遇见 OOM 现象，建议减少max_model_len，或者增加tp_size\n",
        "max_model_len, tp_size = 1024, 1\n",
        "gpu_memory_utilization = 0.5\n",
        "model_name = \"THUDM/glm-4-9b-chat\"\n",
        "prompt = [{\"role\": \"user\", \"content\": \"列举与下面句子最符合的五个成语。只需要输出五个成语，不需要有其他的输出，写在一行中：比喻夫妻关系和谐\"}]\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "llm = LLM(\n",
        "    model=model_name,\n",
        "    tensor_parallel_size=tp_size,\n",
        "    max_model_len=max_model_len,\n",
        "    trust_remote_code=True,\n",
        "    enforce_eager=True,\n",
        "    # GLM-4-9B-Chat-1M 如果遇见 OOM 现象，建议开启下述参数\n",
        "    # enable_chunked_prefill=True,\n",
        "    # max_num_batched_tokens=8192\n",
        ")\n",
        "stop_token_ids = [151329, 151336, 151338]\n",
        "sampling_params = SamplingParams(temperature=0.95, max_tokens=512, stop_token_ids=stop_token_ids)\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
        "outputs = llm.generate(prompts=inputs, sampling_params=sampling_params)\n",
        "\n",
        "print(outputs[0])\n",
        "print(outputs[0].outputs[0].text)\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fc6f812",
      "metadata": {
        "id": "9fc6f812"
      },
      "source": [
        "检查输出是否正常\n",
        "\n",
        "原版“max_model_len, tp_size = 131072, 1”会OOM\n",
        "\n",
        "经调整“max_model_len, tp_size = 65536, 1”可以运行\n",
        "\n",
        "占用显存34.2GB\n",
        "\n",
        "##print(outputs[0]):##\n",
        "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.72it/s, est. speed input: 13.80 toks/s, output: 27.59 toks/s]RequestOutput(request_id=0, prompt='[gMASK]<sop><|user|>\\n你好<|assistant|>', prompt_token_ids=[151331, 151333, 151331, 151333, 151336, 198, 109377, 151337], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, ##text='\\n你好👋！很高兴见到你，欢迎问我任何问题。'##, token_ids=(198, 109377, 9281, 239, 233, 6313, 118295, 103810, 98406, 3837, 100940, 106546, 99766, 98622, 1773, 151336), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=151336)], finished=True, metrics=RequestMetrics(arrival_time=1730035203.5149841, last_token_time=1730035203.5149841, first_scheduled_time=1730035203.5178537, first_token_time=1730035203.5585952, time_in_queue=0.0028696060180664062, finished_time=1730035204.0973136, scheduler_time=0.002600323000251592, model_forward_time=None, model_execute_time=None), lora_request=None)\n",
        "\n",
        "##print(outputs[0].outputs[0].text)##\n",
        "\n",
        "你好👋！很高兴见到你，欢迎问我任何问题。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning:\n",
        "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
        "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
        "You will be able to reuse this secret in all of your notebooks.\n",
        "Please note that authentication is recommended but still optional to access public models or datasets.\n",
        "  warnings.warn(\n",
        "tokenizer_config.json: 100%\n",
        " 6.15k/6.15k [00:00<00:00, 501kB/s]\n",
        "tokenization_chatglm.py: 100%\n",
        " 8.99k/8.99k [00:00<00:00, 769kB/s]\n",
        "A new version of the following files was downloaded from https://huggingface.co/THUDM/glm-4-9b-chat:\n",
        "- tokenization_chatglm.py\n",
        ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
        "tokenizer.model: 100%\n",
        " 2.62M/2.62M [00:00<00:00, 12.0MB/s]\n",
        "config.json: 100%\n",
        " 1.44k/1.44k [00:00<00:00, 126kB/s]\n",
        "WARNING 10-28 02:32:16 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
        "INFO 10-28 02:32:16 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='THUDM/glm-4-9b-chat', speculative_config=None, tokenizer='THUDM/glm-4-9b-chat', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=THUDM/glm-4-9b-chat, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n",
        "WARNING 10-28 02:32:17 tokenizer.py:169] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
        "generation_config.json: 100%\n",
        " 207/207 [00:00<00:00, 17.4kB/s]\n",
        "INFO 10-28 02:32:18 model_runner.py:1056] Starting to load model THUDM/glm-4-9b-chat...\n",
        "INFO 10-28 02:32:18 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
        "model-00001-of-00010.safetensors: 100%\n",
        " 1.95G/1.95G [00:46<00:00, 42.8MB/s]\n",
        "model-00003-of-00010.safetensors: 100%\n",
        " 1.97G/1.97G [00:46<00:00, 42.5MB/s]\n",
        "model-00002-of-00010.safetensors: 100%\n",
        " 1.82G/1.82G [00:43<00:00, 41.3MB/s]\n",
        "model-00004-of-00010.safetensors: 100%\n",
        " 1.93G/1.93G [00:46<00:00, 42.8MB/s]\n",
        "model-00005-of-00010.safetensors: 100%\n",
        " 1.82G/1.82G [00:43<00:00, 42.3MB/s]\n",
        "model-00008-of-00010.safetensors: 100%\n",
        " 1.82G/1.82G [00:43<00:00, 42.2MB/s]\n",
        "model-00006-of-00010.safetensors: 100%\n",
        " 1.97G/1.97G [00:47<00:00, 43.3MB/s]\n",
        "model-00007-of-00010.safetensors: 100%\n",
        " 1.93G/1.93G [00:46<00:00, 41.2MB/s]\n",
        "model-00009-of-00010.safetensors: 100%\n",
        " 1.97G/1.97G [00:46<00:00, 41.9MB/s]\n",
        "model-00010-of-00010.safetensors: 100%\n",
        " 1.65G/1.65G [00:39<00:00, 42.2MB/s]\n",
        "model.safetensors.index.json: 100%\n",
        " 29.1k/29.1k [00:00<00:00, 2.23MB/s]\n",
        "Loading safetensors checkpoint shards: 100% Completed | 10/10 [00:05<00:00,  1.87it/s]\n",
        "INFO 10-28 02:33:55 model_runner.py:1067] Loading model weights took 17.5635 GB\n",
        "INFO 10-28 02:33:56 gpu_executor.py:122] # GPU blocks: 26291, # CPU blocks: 6553\n",
        "INFO 10-28 02:33:56 gpu_executor.py:126] Maximum concurrency for 2048 tokens per request: 205.40x\n",
        "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s, est. speed input: 42.02 toks/s, output: 28.01 toks/s]RequestOutput(request_id=0, prompt='[gMASK]<sop><|user|>\\n列举与下面句子最符合的五个成语。只需要输出五个成语，不需要有其他的输出，写在一行中：比喻夫妻关系和谐<|assistant|>', prompt_token_ids=[151331, 151333, 151331, 151333, 151336, 198, 115571, 98381, 101534, 108259, 98430, 100498, 98314, 105053, 109924, 1773, 107073, 102162, 105053, 109924, 3837, 103628, 98318, 106911, 102162, 3837, 99032, 104120, 98351, 98322, 5122, 110573, 102885, 99172, 101938, 151337], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='\\n相敬如宾、琴瑟和鸣、恩爱夫妻、伉俪情深、夫唱妇随', token_ids=(198, 98463, 100619, 98410, 100907, 5373, 101396, 104899, 98327, 102033, 5373, 118219, 102885, 5373, 17406, 231, 123143, 125894, 5373, 99324, 99918, 99991, 98932, 151336), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=151336)], finished=True, metrics=RequestMetrics(arrival_time=1730082839.8494065, last_token_time=1730082839.8494065, first_scheduled_time=1730082839.8528595, first_token_time=1730082839.9011638, time_in_queue=0.0034530162811279297, finished_time=1730082840.709096, scheduler_time=0.003833885999938502, model_forward_time=None, model_execute_time=None), lora_request=None)\n",
        "\n",
        "相敬如宾、琴瑟和鸣、恩爱夫妻、伉俪情深、夫唱妇随"
      ],
      "metadata": {
        "id": "fOr2851W-az_"
      },
      "id": "fOr2851W-az_"
    },
    {
      "cell_type": "markdown",
      "id": "27aeb786",
      "metadata": {
        "id": "27aeb786"
      },
      "source": [
        "## 步骤4：读取数据集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "579d0f7f-a511-4d53-9b6c-a4cd1fcc2b87",
      "metadata": {
        "tags": [],
        "id": "579d0f7f-a511-4d53-9b6c-a4cd1fcc2b87"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "test = pd.read_csv('./test_input.csv', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f67aed74-9059-48a0-b0b8-3750f8ec22fe",
      "metadata": {
        "ExecutionIndicator": {
          "show": true
        },
        "scrolled": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f67aed74-9059-48a0-b0b8-3750f8ec22fe",
        "outputId": "fd4ff79c-2be3-4cbe-bf69-50544668f9cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "数据集的大小为: 2972\n",
            "前50条数据如下：\n",
            "\n",
            "双方心往一处想，意志坚定。\n",
            "两端都极为艰难，难以作出决定。\n",
            "雄浑深远的意旨。细腻微妙。高超巧妙。\n",
            "描述水流湍急，且快速且深远。\n",
            "避免被引诱做出不道德或可疑的行为。\n",
            "肘部肩窝。比喻事物发生于身边。\n",
            "他张开嘴巴，吞咽着口水。\n",
            "仅见一面，不足以深入了解。\n",
            "向四处散发施舍，同时手中拿着碗盆。\n",
            "比喻把装备脱下，放下武器。\n",
            "对于任何战役，都要一败涂地。\n",
            "阴谋真相已经昭然于众。\n",
            "比喻因为无能为力而写下了好文章。\n",
            "古代文人士大夫经常举行诗歌朗诵会。\n",
            "不能形容为不高兴，只能说明没劲儿。\n",
            "表现得举止端庄，很有教养。\n",
            "1.洗刷兵器2.喂养战马3.准备作战\n",
            "关注生命垂危者，关怀濒危者。\n",
            "比喻面对挑战，坚韧不拔地前行。\n",
            "过去的科举考试中被选拔为进士的称号。\n",
            "相似程度极高或相差无几。\n",
            "比喻不断地补充、堆砌和延伸。\n",
            "以安逸快乐的生活和劳动为重。\n",
            "她仍然每天教导她的儿子。\n",
            "犹以火为耕，比喻原始、简朴的农耕方式。\n",
            "指困境中处于不利地位。\n",
            "搜集和研究其内在道理。\n",
            "没有任何人帮助和支持。\n",
            "这位作者的文章风格与自己非常相似。\n",
            "国力强大，军事力量已经停止。\n",
            "相互勾结维持；相互利用。\n",
            "比喻进程飞快，日行千里。\n",
            "只有一个目的，追求利润。\n",
            "务必谨慎对待，慎重处理事务。\n",
            "无法言表，只能感慨万千。\n",
            "形容气势磅礴的文章风格。\n",
            "根据贡献的大小给予奖励。\n",
            "让国家蒙羞，民众蒙难。\n",
            "形容有权势的人极其残忍和无礼。\n",
            "坚决要求再次向某人强调。\n",
            "采取措施；采取办法；采取行动；实行\n",
            "心中充满了疑惑，还没有找到解答。\n",
            "累累罪行，遍历无穷。形容罪恶极重。\n",
            "形容人的容貌清爽俊雅，风度翩翩。\n",
            "辞掉本职工作去做其他的事情。\n",
            "秦汉时期，勋位至高者都佩戴金印和紫绶。\n",
            "创立独特的风格，与众不同。\n",
            "到处都是冰雪覆盖的环境，形容严冬天气。\n",
            "旧事物被废弃；为了新事物而采取措施。\n",
            "①向人行礼。②用作哀悼词或祭奠语。\n"
          ]
        }
      ],
      "source": [
        "# 查看数据集大小\n",
        "print(f\"数据集的大小为: {test.shape[0]}\\n前50条数据如下：\\n\")\n",
        "\n",
        "# 查看前50条赛事数据集（赛题要求根据每行句子，给出5个可能匹配的成语）\n",
        "for test_prompt in test[0].values[:50]:\n",
        "    print(test_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a5fbc56-26d4-4131-b8f9-452899ead64b",
      "metadata": {
        "id": "1a5fbc56-26d4-4131-b8f9-452899ead64b"
      },
      "source": [
        "## 步骤5：输出成语"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "爆显存(out of memory）比较严重，目前Colab A100单张40GB显存还不够用。期待大佬的意见。"
      ],
      "metadata": {
        "id": "VAf99SvFRkwh"
      },
      "id": "VAf99SvFRkwh"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "154ae267-49b5-4250-b0ad-c89b5c7ff3da",
      "metadata": {
        "ExecutionIndicator": {
          "show": true
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "154ae267-49b5-4250-b0ad-c89b5c7ff3da",
        "outputId": "ac9043a5-7f28-4233-f5f1-198599ab6bf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r处理进度:   0%|          | 0/2972 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 10-29 03:15:41 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
            "INFO 10-29 03:15:41 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='THUDM/glm-4-9b-chat', speculative_config=None, tokenizer='THUDM/glm-4-9b-chat', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=THUDM/glm-4-9b-chat, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n",
            "WARNING 10-29 03:15:42 tokenizer.py:169] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
            "INFO 10-29 03:15:42 model_runner.py:1056] Starting to load model THUDM/glm-4-9b-chat...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r处理进度:   0%|          | 0/2972 [00:02<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 39.56 GiB of which 26.81 MiB is free. Process 105528 has 39.52 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 60.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-d0b24688168a>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# 初始化大语言模型(LLM)，配置模型并行大小、最大模型长度等参数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     llm = LLM(\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtensor_parallel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtp_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, mm_processor_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         )\n\u001b[0;32m--> 177\u001b[0;31m         self.llm_engine = LLMEngine.from_engine_args(\n\u001b[0m\u001b[1;32m    178\u001b[0m             engine_args, usage_context=UsageContext.LLM_CLASS)\n\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36mfrom_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0mexecutor_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_executor_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;31m# Create the LLM engine.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m         engine = cls(\n\u001b[0m\u001b[1;32m    574\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mengine_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mexecutor_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecutor_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, speculative_config, decoding_config, observability_config, prompt_adapter_config, executor_class, log_stats, usage_context, stat_loggers, input_registry, use_cached_outputs)\u001b[0m\n\u001b[1;32m    332\u001b[0m             model_config)\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         self.model_executor = executor_class(\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0mmodel_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mcache_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/executor/executor_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, speculative_config, prompt_adapter_config, observability_config)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_adapter_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_adapter_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservability_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservability_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_executor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\u001b[0m in \u001b[0;36m_init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver_worker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver_worker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver_worker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     def _get_worker_kwargs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     def save_sharded_state(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1056\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting to load model %s...\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mDeviceMemoryProfiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m             self.model = get_model(model_config=self.model_config,\n\u001b[0m\u001b[1;32m   1059\u001b[0m                                    \u001b[0mdevice_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m                                    \u001b[0mload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/__init__.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(model_config, load_config, device_config, parallel_config, scheduler_config, lora_config, cache_config)\u001b[0m\n\u001b[1;32m     17\u001b[0m               cache_config: CacheConfig) -> nn.Module:\n\u001b[1;32m     18\u001b[0m     \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     return loader.load_model(model_config=model_config,\n\u001b[0m\u001b[1;32m     20\u001b[0m                              \u001b[0mdevice_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                              \u001b[0mlora_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlora_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, model_config, device_config, lora_config, parallel_config, scheduler_config, cache_config)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mset_default_torch_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtarget_device\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m                 model = _initialize_model(model_config, self.load_config,\n\u001b[0m\u001b[1;32m    399\u001b[0m                                           \u001b[0mlora_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m                                           scheduler_config)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\u001b[0m in \u001b[0;36m_initialize_model\u001b[0;34m(model_config, load_config, lora_config, cache_config, scheduler_config)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_architecture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     return build_model(\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhf_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(model_class, hf_config, cache_config, quant_config, lora_config, multimodal_config, scheduler_config)\u001b[0m\n\u001b[1;32m    158\u001b[0m                                                     scheduler_config)\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     return model_class(config=hf_config,\n\u001b[0m\u001b[1;32m    161\u001b[0m                        \u001b[0mcache_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                        \u001b[0mquant_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquant_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/chatglm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, multimodal_config, cache_config, quant_config, lora_config)\u001b[0m\n\u001b[1;32m    595\u001b[0m         self.max_position_embeddings = getattr(config, \"max_sequence_length\",\n\u001b[1;32m    596\u001b[0m                                                8192)\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatGLMModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtie_word_embeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m             self.transformer.output_layer.weight = (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/chatglm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, cache_config, quant_config)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m         self.embedding = VocabParallelEmbedding(config.padded_vocab_size,\n\u001b[0m\u001b[1;32m    488\u001b[0m                                                 \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                                                 quant_config=quant_config)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/vocab_parallel_embedding.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_embeddings, embedding_dim, params_dtype, org_num_embeddings, padding_size, quant_config, prefix)\u001b[0m\n\u001b[1;32m    258\u001b[0m             self.shard_indices.added_vocab_start_index)\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         self.linear_method.create_weights(self,\n\u001b[0m\u001b[1;32m    261\u001b[0m                                           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                                           \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_embeddings_per_partition\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/vocab_parallel_embedding.py\u001b[0m in \u001b[0;36mcreate_weights\u001b[0;34m(self, layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype, **extra_weight_attrs)\u001b[0m\n\u001b[1;32m     26\u001b[0m                        **extra_weight_attrs):\n\u001b[1;32m     27\u001b[0m         \u001b[0;34m\"\"\"Create weights for embedding layer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         weight = Parameter(torch.empty(sum(output_partition_sizes),\n\u001b[0m\u001b[1;32m     29\u001b[0m                                        \u001b[0minput_size_per_partition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                                        dtype=params_dtype),\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_device_constructors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# NB: This is directly called from C++ in torch/csrc/Device.cpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 39.56 GiB of which 26.81 MiB is free. Process 105528 has 39.52 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 60.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "\n",
        "i = 1\n",
        "# 假设 test 是一个 DataFrame\n",
        "# 遍历测试数据集的第一项的值，目的是生成与给定句子最相关的五个成语\n",
        "for test_prompt in tqdm(test[0].values, total=len(test[0].values), desc=\"处理进度\"):\n",
        "    i = i + 1\n",
        "    # 构造提示信息，要求模型输出与句子最相关的五个成语\n",
        "    prompt = [{\"role\": \"user\", \"content\": f\"列举与下面句子最符合的五个成语。只需要输出五个成语，不需要有其他的输出，写在一行中：{test_prompt}\"}]\n",
        "\n",
        "    # 初始化一个长度为5的列表，填充默认成语“同舟共济”\n",
        "    words = ['同舟共济'] * 5\n",
        "\n",
        "    # 使用预训练模型初始化tokenizer，信任远程代码以支持可能的自定义实现\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "    # 初始化大语言模型(LLM)，配置模型并行大小、最大模型长度等参数\n",
        "    llm = LLM(\n",
        "    model=model_name,\n",
        "    tensor_parallel_size=tp_size,\n",
        "    max_model_len=max_model_len,\n",
        "    trust_remote_code=True,\n",
        "    enforce_eager=True,\n",
        "    gpu_memory_utilization=0.8\n",
        "\n",
        "    )\n",
        "\n",
        "    # 定义停止生成的token ID列表，用于控制生成文本的结束\n",
        "    stop_token_ids = [151329, 151336, 151338]\n",
        "    # 设置采样参数，包括温度、最大生成token数和停止token ID\n",
        "    sampling_params = SamplingParams(temperature=0.95, max_tokens=1024, stop_token_ids=stop_token_ids)\n",
        "    # 使用tokenizer将提示转化为模型所需的输入格式，不进行token化，添加生成提示\n",
        "    inputs = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
        "    # 生成文本输出，根据输入提示和采样参数进行文本生成\n",
        "    outputs = llm.generate(prompts=inputs, sampling_params=sampling_params)\n",
        "\n",
        "    # 解码模型输出，去除特殊标记\n",
        "    response = outputs[0].outputs[0].text\n",
        "\n",
        "    # 清理回答文本，确保格式统一 需要去掉的内容包括常规标点符号，其中'\\n'是换行符，使输出回到同一行内\n",
        "    response = response.replace('\\n', ' ').replace('、', ' ').replace(',', ' ').replace('；',' ').replace('。',' ')\n",
        "    # 提取回答中的成语，确保每个成语长度为4且非空\n",
        "    words = [x for x in response.split() if len(x) == 4 and x.strip() != '']\n",
        "\n",
        "\n",
        "    # 如果生成的成语列表长度不满足要求（即20个字符），则使用默认成语列表\n",
        "   #if len(' '.join(words).strip()) != 24:\n",
        "       # words = ['同舟共济'] * 5\n",
        "    while True:\n",
        "        text = ' '.join(words).strip()\n",
        "        if len(text) < 24:\n",
        "            words.append('同舟共济')\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # 将最终的成语列表写入提交文件\n",
        "    with open('submit.csv', 'a+', encoding='utf-8') as up:\n",
        "        up.write(' '.join(words) + '\\n')\n",
        "\n",
        "\n",
        "    # 查看阶段性结果\n",
        "    if i % 50 == 0:\n",
        "        tqdm.write(f\"大模型第{i}次返回的结果是：\\n   {response}\\n\")\n",
        "        tqdm.write(f\"submit.cvs第{i}行输出结果：\\n   {words}\\n\")\n",
        "\n",
        "    # 为了尽快拿到结果，我们暂时仅获得500个结果（如果有时间的话，可以删除这两行）\n",
        "    if i == 2973:\n",
        "        break\n",
        "\n",
        "print('submit.csv 已生成')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "891607d15775446ca8f6c518bf335673": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_723a52242bac430b97a607b186dddc74",
              "IPY_MODEL_ce19752078d6444dbe80f73045477346",
              "IPY_MODEL_e83363cb2184413590238f99f62667d3"
            ],
            "layout": "IPY_MODEL_075ae5356a1d4358a0b86205b15d4f49"
          }
        },
        "723a52242bac430b97a607b186dddc74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3dd29182748439ea862b822b6d81a49",
            "placeholder": "​",
            "style": "IPY_MODEL_1b3b1720befe41fa90d423d7dd4c5731",
            "value": ""
          }
        },
        "ce19752078d6444dbe80f73045477346": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cad687a7b5084c8ca7e2300090e93e7c",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2465db233c024598a621a68e0e986222",
            "value": 10
          }
        },
        "e83363cb2184413590238f99f62667d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34393296c08c40e2b0eb3c7888af9136",
            "placeholder": "​",
            "style": "IPY_MODEL_add46b1cf2724f7e9aec0ffe280f72a8",
            "value": "Loading safetensors checkpoint shards: 100% Completed | 10/10 [00:05&lt;00:00,  1.84it/s]\n"
          }
        },
        "075ae5356a1d4358a0b86205b15d4f49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3dd29182748439ea862b822b6d81a49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b3b1720befe41fa90d423d7dd4c5731": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cad687a7b5084c8ca7e2300090e93e7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2465db233c024598a621a68e0e986222": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "34393296c08c40e2b0eb3c7888af9136": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "add46b1cf2724f7e9aec0ffe280f72a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}