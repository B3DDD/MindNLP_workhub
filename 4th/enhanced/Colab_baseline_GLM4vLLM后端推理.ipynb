{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "18a8de8f",
      "metadata": {
        "id": "18a8de8f"
      },
      "source": [
        "# åŸºç¡€æ€è·¯ï¼ˆbaselineï¼‰"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5861209f",
      "metadata": {
        "id": "5861209f"
      },
      "source": [
        "å› ä¸ºtransformeråç«¯æ¨ç†ååˆ†ç¼“æ…¢ï¼Œå¯¼è‡´äº†å¯¹è®¡ç®—å•å…ƒï¼ˆæ ¸æ—¶ï¼‰çš„æ¶ˆè€—éå¸¸å¤§ã€‚å°è¯•ä½¿ç”¨vLLMå¯¹GLM4-9B-CHATæ¨¡å‹è¿›è¡Œåç«¯æ¨ç†ï¼Œä»¥å‡å°‘å¯¹è®¡ç®—å•å…ƒçš„æ¶ˆè€—ã€‚\n",
        "\n",
        "vLLMçš„æ¨ç†æ˜¯å¢åŠ ååé‡çš„æ–¹å¼ï¼Œå¯¹äºæ˜¾å­˜çš„ä½¿ç”¨ç­–ç•¥éå¸¸æ¿€è¿›ï¼Œä¼šå æ®å¤§é‡çš„æ˜¾å­˜ç©ºé—´ï¼Œè¯·ä¿è¯è‡³å°‘36GBä»¥ä¸Šçš„æ˜¾å­˜ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa932cfc",
      "metadata": {
        "id": "fa932cfc"
      },
      "source": [
        "## æ­¥éª¤1ï¼šæ›´æ–°æˆ–å®‰è£…æ‰€éœ€ç¯å¢ƒ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6724776c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6724776c",
        "outputId": "0ece773e-ca50-48f1-ecdc-1a8c607b25b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (2.2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Collecting tiktoken\n",
            "  Using cached tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: vllm in /usr/local/lib/python3.10/dist-packages (0.6.3.post1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from vllm) (5.9.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from vllm) (0.2.0)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from vllm) (9.0.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from vllm) (3.20.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from vllm) (3.10.10)\n",
            "Requirement already satisfied: openai>=1.40.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (1.52.2)\n",
            "Requirement already satisfied: uvicorn[standard] in /usr/local/lib/python3.10/dist-packages (from vllm) (0.32.0)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.10/dist-packages (from vllm) (2.9.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from vllm) (10.4.0)\n",
            "Requirement already satisfied: prometheus-client>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.21.0)\n",
            "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (7.0.0)\n",
            "Requirement already satisfied: lm-format-enforcer==0.10.6 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.10.6)\n",
            "Requirement already satisfied: outlines<0.1,>=0.0.43 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.0.46)\n",
            "Requirement already satisfied: typing-extensions>=4.10 in /usr/local/lib/python3.10/dist-packages (from vllm) (4.12.2)\n",
            "Requirement already satisfied: partial-json-parser in /usr/local/lib/python3.10/dist-packages (from vllm) (0.2.1.1.post4)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.10/dist-packages (from vllm) (24.0.1)\n",
            "Requirement already satisfied: msgspec in /usr/local/lib/python3.10/dist-packages (from vllm) (0.18.6)\n",
            "Requirement already satisfied: gguf==0.10.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.10.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from vllm) (8.5.0)\n",
            "Requirement already satisfied: mistral-common>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from mistral-common[opencv]>=1.4.4->vllm) (1.4.4)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from vllm) (0.8.0)\n",
            "Requirement already satisfied: compressed-tensors==0.6.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.6.0)\n",
            "Requirement already satisfied: ray>=2.9 in /usr/local/lib/python3.10/dist-packages (from vllm) (2.38.0)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.10/dist-packages (from vllm) (12.560.30)\n",
            "Requirement already satisfied: torch==2.4.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (2.4.0)\n",
            "Requirement already satisfied: torchvision==0.19 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.19.0)\n",
            "Requirement already satisfied: xformers==0.0.27.post2 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.0.27.post2)\n",
            "Requirement already satisfied: fastapi!=0.113.*,!=0.114.0,>=0.107.0 in /usr/local/lib/python3.10/dist-packages (from vllm) (0.115.4)\n",
            "Requirement already satisfied: interegular>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from lm-format-enforcer==0.10.6->vllm) (0.3.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->vllm) (12.6.77)\n",
            "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm) (0.41.2)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.21.1 in /usr/local/lib/python3.10/dist-packages (from mistral-common>=1.4.4->mistral-common[opencv]>=1.4.4->vllm) (4.23.0)\n",
            "Requirement already satisfied: opencv-python-headless<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mistral-common[opencv]>=1.4.4->vllm) (4.10.0.84)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->vllm) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.40.0->vllm) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->vllm) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->vllm) (0.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.40.0->vllm) (1.3.1)\n",
            "Requirement already satisfied: lark in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm) (1.2.2)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm) (1.6.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm) (3.1.0)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm) (5.6.3)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm) (0.60.0)\n",
            "Requirement already satisfied: referencing in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm) (0.35.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm) (3.0.2)\n",
            "Requirement already satisfied: pycountry in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm) (24.6.1)\n",
            "Requirement already satisfied: pyairports in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm) (2.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9->vllm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9->vllm) (2.23.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (8.1.7)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (1.1.0)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm) (1.4.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (2.4.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (24.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm) (4.0.3)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->vllm) (3.20.2)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm) (0.24.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm) (13.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.40.0->vllm) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.40.0->vllm) (1.0.6)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common>=1.4.4->mistral-common[opencv]>=1.4.4->vllm) (2024.10.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common>=1.4.4->mistral-common[opencv]>=1.4.4->vllm) (0.20.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->vllm) (0.2.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm) (0.70.16)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0->vllm) (3.0.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->outlines<0.1,>=0.0.43->vllm) (0.43.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0->vllm) (1.3.0)\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers requests urllib3 tqdm pandas tiktoken vllm\n",
        "!apt update > /dev/null; apt install aria2 git-lfs axel -y > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d22db0c3",
      "metadata": {
        "id": "d22db0c3"
      },
      "source": [
        "å¢åŠ äº† transformers ã€tiktokenã€vllm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "683b4361",
      "metadata": {
        "id": "683b4361"
      },
      "source": [
        "## æ­¥éª¤2ï¼šä¸‹è½½æ•°æ®é›†"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5e631c84",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-22T05:26:17.398404Z",
          "iopub.status.busy": "2024-10-22T05:26:17.398143Z",
          "iopub.status.idle": "2024-10-22T05:26:17.773504Z",
          "shell.execute_reply": "2024-10-22T05:26:17.772988Z",
          "shell.execute_reply.started": "2024-10-22T05:26:17.398385Z"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e631c84",
        "outputId": "e5bb3ce8-aa78-4786-a1e4-80f340195a0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing download: https://ai-contest-static.xfyun.cn/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%83%BD%E5%8A%9B%E8%AF%84%E6%B5%8B%EF%BC%9A%E4%B8%AD%E6%96%87%E6%88%90%E8%AF%AD%E9%87%8A%E4%B9%89%E4%B8%8E%E8%A7%A3%E6%9E%90%E6%8C%91%E6%88%98%E8%B5%9B/test_input.csv\n",
            "File size: 131.332 Kilobyte(s) (134484 bytes)\n",
            "Opening output file test_input.csv.2\n",
            "Starting download\n",
            "\n",
            "Can't setup alternate output. Deactivating.\n",
            "..... .......... .......... .......... ..........  [ 259.8KB/s]\n",
            "[ 38%]  .......... .......... .......... .......... ..........  [ 467.3KB/s]\n",
            "[ 76%]  .......... .......... .......... .\n",
            "\n",
            "Downloaded 131.332 Kilobyte(s) in 0 second(s). (547.49 KB/s)\n"
          ]
        }
      ],
      "source": [
        "!axel -n 12 -a https://ai-contest-static.xfyun.cn/2024/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%83%BD%E5%8A%9B%E8%AF%84%E6%B5%8B%EF%BC%9A%E4%B8%AD%E6%96%87%E6%88%90%E8%AF%AD%E9%87%8A%E4%B9%89%E4%B8%8E%E8%A7%A3%E6%9E%90%E6%8C%91%E6%88%98%E8%B5%9B/test_input.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e13b85fa",
      "metadata": {
        "id": "e13b85fa"
      },
      "source": [
        "## æ­¥éª¤3ï¼šæ„å»ºæ¨¡å‹ï¼ˆä½¿ç”¨GLM-4-9B-Chatï¼‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5c0e04dd",
      "metadata": {
        "ExecutionIndicator": {
          "show": true
        },
        "execution": {
          "iopub.execute_input": "2024-10-22T05:26:17.774571Z",
          "iopub.status.busy": "2024-10-22T05:26:17.774318Z"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381,
          "referenced_widgets": [
            "891607d15775446ca8f6c518bf335673",
            "723a52242bac430b97a607b186dddc74",
            "ce19752078d6444dbe80f73045477346",
            "e83363cb2184413590238f99f62667d3",
            "075ae5356a1d4358a0b86205b15d4f49",
            "f3dd29182748439ea862b822b6d81a49",
            "1b3b1720befe41fa90d423d7dd4c5731",
            "cad687a7b5084c8ca7e2300090e93e7c",
            "2465db233c024598a621a68e0e986222",
            "34393296c08c40e2b0eb3c7888af9136",
            "add46b1cf2724f7e9aec0ffe280f72a8"
          ]
        },
        "id": "5c0e04dd",
        "outputId": "05224ca6-90cd-4b09-ec12-a6d9141b26a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 10-29 03:09:00 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
            "INFO 10-29 03:09:00 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='THUDM/glm-4-9b-chat', speculative_config=None, tokenizer='THUDM/glm-4-9b-chat', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=THUDM/glm-4-9b-chat, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n",
            "WARNING 10-29 03:09:01 tokenizer.py:169] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
            "INFO 10-29 03:09:01 model_runner.py:1056] Starting to load model THUDM/glm-4-9b-chat...\n",
            "INFO 10-29 03:09:02 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/10 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "891607d15775446ca8f6c518bf335673"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 10-29 03:09:08 model_runner.py:1067] Loading model weights took 17.5635 GB\n",
            "INFO 10-29 03:09:09 gpu_executor.py:122] # GPU blocks: 26291, # CPU blocks: 6553\n",
            "INFO 10-29 03:09:09 gpu_executor.py:126] Maximum concurrency for 1024 tokens per request: 410.80x\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.18it/s, est. speed input: 42.67 toks/s, output: 28.44 toks/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RequestOutput(request_id=0, prompt='[gMASK]<sop><|user|>\\nåˆ—ä¸¾ä¸ä¸‹é¢å¥å­æœ€ç¬¦åˆçš„äº”ä¸ªæˆè¯­ã€‚åªéœ€è¦è¾“å‡ºäº”ä¸ªæˆè¯­ï¼Œä¸éœ€è¦æœ‰å…¶ä»–çš„è¾“å‡ºï¼Œå†™åœ¨ä¸€è¡Œä¸­ï¼šæ¯”å–»å¤«å¦»å…³ç³»å’Œè°<|assistant|>', prompt_token_ids=[151331, 151333, 151331, 151333, 151336, 198, 115571, 98381, 101534, 108259, 98430, 100498, 98314, 105053, 109924, 1773, 107073, 102162, 105053, 109924, 3837, 103628, 98318, 106911, 102162, 3837, 99032, 104120, 98351, 98322, 5122, 110573, 102885, 99172, 101938, 151337], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='\\nç›¸æ•¬å¦‚å®¾ã€ç´ç‘Ÿå’Œé¸£ã€æ©çˆ±å¤«å¦»ã€ä¼‰ä¿ªæƒ…æ·±ã€å¤«å”±å¦‡éš', token_ids=(198, 98463, 100619, 98410, 100907, 5373, 101396, 104899, 98327, 102033, 5373, 118219, 102885, 5373, 17406, 231, 123143, 125894, 5373, 99324, 99918, 99991, 98932, 151336), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=151336)], finished=True, metrics=RequestMetrics(arrival_time=1730171352.6092703, last_token_time=1730171352.6092703, first_scheduled_time=1730171352.6124427, first_token_time=1730171352.6596408, time_in_queue=0.0031723976135253906, finished_time=1730171353.4557714, scheduler_time=0.0033692079998672853, model_forward_time=None, model_execute_time=None), lora_request=None)\n",
            "\n",
            "ç›¸æ•¬å¦‚å®¾ã€ç´ç‘Ÿå’Œé¸£ã€æ©çˆ±å¤«å¦»ã€ä¼‰ä¿ªæƒ…æ·±ã€å¤«å”±å¦‡éš\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from vllm import LLM, SamplingParams\n",
        "import torch\n",
        "\n",
        "# GLM-4-9B-Chat-1M\n",
        "# max_model_len, tp_size = 1048576, 4\n",
        "\n",
        "# GLM-4-9B-Chat\n",
        "# å¦‚æœé‡è§ OOM ç°è±¡ï¼Œå»ºè®®å‡å°‘max_model_lenï¼Œæˆ–è€…å¢åŠ tp_size\n",
        "max_model_len, tp_size = 1024, 1\n",
        "gpu_memory_utilization = 0.5\n",
        "model_name = \"THUDM/glm-4-9b-chat\"\n",
        "prompt = [{\"role\": \"user\", \"content\": \"åˆ—ä¸¾ä¸ä¸‹é¢å¥å­æœ€ç¬¦åˆçš„äº”ä¸ªæˆè¯­ã€‚åªéœ€è¦è¾“å‡ºäº”ä¸ªæˆè¯­ï¼Œä¸éœ€è¦æœ‰å…¶ä»–çš„è¾“å‡ºï¼Œå†™åœ¨ä¸€è¡Œä¸­ï¼šæ¯”å–»å¤«å¦»å…³ç³»å’Œè°\"}]\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "llm = LLM(\n",
        "    model=model_name,\n",
        "    tensor_parallel_size=tp_size,\n",
        "    max_model_len=max_model_len,\n",
        "    trust_remote_code=True,\n",
        "    enforce_eager=True,\n",
        "    # GLM-4-9B-Chat-1M å¦‚æœé‡è§ OOM ç°è±¡ï¼Œå»ºè®®å¼€å¯ä¸‹è¿°å‚æ•°\n",
        "    # enable_chunked_prefill=True,\n",
        "    # max_num_batched_tokens=8192\n",
        ")\n",
        "stop_token_ids = [151329, 151336, 151338]\n",
        "sampling_params = SamplingParams(temperature=0.95, max_tokens=512, stop_token_ids=stop_token_ids)\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
        "outputs = llm.generate(prompts=inputs, sampling_params=sampling_params)\n",
        "\n",
        "print(outputs[0])\n",
        "print(outputs[0].outputs[0].text)\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fc6f812",
      "metadata": {
        "id": "9fc6f812"
      },
      "source": [
        "æ£€æŸ¥è¾“å‡ºæ˜¯å¦æ­£å¸¸\n",
        "\n",
        "åŸç‰ˆâ€œmax_model_len, tp_size = 131072, 1â€ä¼šOOM\n",
        "\n",
        "ç»è°ƒæ•´â€œmax_model_len, tp_size = 65536, 1â€å¯ä»¥è¿è¡Œ\n",
        "\n",
        "å ç”¨æ˜¾å­˜34.2GB\n",
        "\n",
        "##print(outputs[0]):##\n",
        "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.72it/s, est. speed input: 13.80 toks/s, output: 27.59 toks/s]RequestOutput(request_id=0, prompt='[gMASK]<sop><|user|>\\nä½ å¥½<|assistant|>', prompt_token_ids=[151331, 151333, 151331, 151333, 151336, 198, 109377, 151337], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, ##text='\\nä½ å¥½ğŸ‘‹ï¼å¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚'##, token_ids=(198, 109377, 9281, 239, 233, 6313, 118295, 103810, 98406, 3837, 100940, 106546, 99766, 98622, 1773, 151336), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=151336)], finished=True, metrics=RequestMetrics(arrival_time=1730035203.5149841, last_token_time=1730035203.5149841, first_scheduled_time=1730035203.5178537, first_token_time=1730035203.5585952, time_in_queue=0.0028696060180664062, finished_time=1730035204.0973136, scheduler_time=0.002600323000251592, model_forward_time=None, model_execute_time=None), lora_request=None)\n",
        "\n",
        "##print(outputs[0].outputs[0].text)##\n",
        "\n",
        "ä½ å¥½ğŸ‘‹ï¼å¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning:\n",
        "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
        "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
        "You will be able to reuse this secret in all of your notebooks.\n",
        "Please note that authentication is recommended but still optional to access public models or datasets.\n",
        "  warnings.warn(\n",
        "tokenizer_config.json:â€‡100%\n",
        "â€‡6.15k/6.15kâ€‡[00:00<00:00,â€‡501kB/s]\n",
        "tokenization_chatglm.py:â€‡100%\n",
        "â€‡8.99k/8.99kâ€‡[00:00<00:00,â€‡769kB/s]\n",
        "A new version of the following files was downloaded from https://huggingface.co/THUDM/glm-4-9b-chat:\n",
        "- tokenization_chatglm.py\n",
        ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
        "tokenizer.model:â€‡100%\n",
        "â€‡2.62M/2.62Mâ€‡[00:00<00:00,â€‡12.0MB/s]\n",
        "config.json:â€‡100%\n",
        "â€‡1.44k/1.44kâ€‡[00:00<00:00,â€‡126kB/s]\n",
        "WARNING 10-28 02:32:16 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
        "INFO 10-28 02:32:16 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='THUDM/glm-4-9b-chat', speculative_config=None, tokenizer='THUDM/glm-4-9b-chat', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=THUDM/glm-4-9b-chat, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n",
        "WARNING 10-28 02:32:17 tokenizer.py:169] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
        "generation_config.json:â€‡100%\n",
        "â€‡207/207â€‡[00:00<00:00,â€‡17.4kB/s]\n",
        "INFO 10-28 02:32:18 model_runner.py:1056] Starting to load model THUDM/glm-4-9b-chat...\n",
        "INFO 10-28 02:32:18 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
        "model-00001-of-00010.safetensors:â€‡100%\n",
        "â€‡1.95G/1.95Gâ€‡[00:46<00:00,â€‡42.8MB/s]\n",
        "model-00003-of-00010.safetensors:â€‡100%\n",
        "â€‡1.97G/1.97Gâ€‡[00:46<00:00,â€‡42.5MB/s]\n",
        "model-00002-of-00010.safetensors:â€‡100%\n",
        "â€‡1.82G/1.82Gâ€‡[00:43<00:00,â€‡41.3MB/s]\n",
        "model-00004-of-00010.safetensors:â€‡100%\n",
        "â€‡1.93G/1.93Gâ€‡[00:46<00:00,â€‡42.8MB/s]\n",
        "model-00005-of-00010.safetensors:â€‡100%\n",
        "â€‡1.82G/1.82Gâ€‡[00:43<00:00,â€‡42.3MB/s]\n",
        "model-00008-of-00010.safetensors:â€‡100%\n",
        "â€‡1.82G/1.82Gâ€‡[00:43<00:00,â€‡42.2MB/s]\n",
        "model-00006-of-00010.safetensors:â€‡100%\n",
        "â€‡1.97G/1.97Gâ€‡[00:47<00:00,â€‡43.3MB/s]\n",
        "model-00007-of-00010.safetensors:â€‡100%\n",
        "â€‡1.93G/1.93Gâ€‡[00:46<00:00,â€‡41.2MB/s]\n",
        "model-00009-of-00010.safetensors:â€‡100%\n",
        "â€‡1.97G/1.97Gâ€‡[00:46<00:00,â€‡41.9MB/s]\n",
        "model-00010-of-00010.safetensors:â€‡100%\n",
        "â€‡1.65G/1.65Gâ€‡[00:39<00:00,â€‡42.2MB/s]\n",
        "model.safetensors.index.json:â€‡100%\n",
        "â€‡29.1k/29.1kâ€‡[00:00<00:00,â€‡2.23MB/s]\n",
        "Loadingâ€‡safetensorsâ€‡checkpointâ€‡shards:â€‡100%â€‡Completedâ€‡|â€‡10/10â€‡[00:05<00:00,â€‡â€‡1.87it/s]\n",
        "INFO 10-28 02:33:55 model_runner.py:1067] Loading model weights took 17.5635 GB\n",
        "INFO 10-28 02:33:56 gpu_executor.py:122] # GPU blocks: 26291, # CPU blocks: 6553\n",
        "INFO 10-28 02:33:56 gpu_executor.py:126] Maximum concurrency for 2048 tokens per request: 205.40x\n",
        "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.17it/s, est. speed input: 42.02 toks/s, output: 28.01 toks/s]RequestOutput(request_id=0, prompt='[gMASK]<sop><|user|>\\nåˆ—ä¸¾ä¸ä¸‹é¢å¥å­æœ€ç¬¦åˆçš„äº”ä¸ªæˆè¯­ã€‚åªéœ€è¦è¾“å‡ºäº”ä¸ªæˆè¯­ï¼Œä¸éœ€è¦æœ‰å…¶ä»–çš„è¾“å‡ºï¼Œå†™åœ¨ä¸€è¡Œä¸­ï¼šæ¯”å–»å¤«å¦»å…³ç³»å’Œè°<|assistant|>', prompt_token_ids=[151331, 151333, 151331, 151333, 151336, 198, 115571, 98381, 101534, 108259, 98430, 100498, 98314, 105053, 109924, 1773, 107073, 102162, 105053, 109924, 3837, 103628, 98318, 106911, 102162, 3837, 99032, 104120, 98351, 98322, 5122, 110573, 102885, 99172, 101938, 151337], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='\\nç›¸æ•¬å¦‚å®¾ã€ç´ç‘Ÿå’Œé¸£ã€æ©çˆ±å¤«å¦»ã€ä¼‰ä¿ªæƒ…æ·±ã€å¤«å”±å¦‡éš', token_ids=(198, 98463, 100619, 98410, 100907, 5373, 101396, 104899, 98327, 102033, 5373, 118219, 102885, 5373, 17406, 231, 123143, 125894, 5373, 99324, 99918, 99991, 98932, 151336), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=151336)], finished=True, metrics=RequestMetrics(arrival_time=1730082839.8494065, last_token_time=1730082839.8494065, first_scheduled_time=1730082839.8528595, first_token_time=1730082839.9011638, time_in_queue=0.0034530162811279297, finished_time=1730082840.709096, scheduler_time=0.003833885999938502, model_forward_time=None, model_execute_time=None), lora_request=None)\n",
        "\n",
        "ç›¸æ•¬å¦‚å®¾ã€ç´ç‘Ÿå’Œé¸£ã€æ©çˆ±å¤«å¦»ã€ä¼‰ä¿ªæƒ…æ·±ã€å¤«å”±å¦‡éš"
      ],
      "metadata": {
        "id": "fOr2851W-az_"
      },
      "id": "fOr2851W-az_"
    },
    {
      "cell_type": "markdown",
      "id": "27aeb786",
      "metadata": {
        "id": "27aeb786"
      },
      "source": [
        "## æ­¥éª¤4ï¼šè¯»å–æ•°æ®é›†"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "579d0f7f-a511-4d53-9b6c-a4cd1fcc2b87",
      "metadata": {
        "tags": [],
        "id": "579d0f7f-a511-4d53-9b6c-a4cd1fcc2b87"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "test = pd.read_csv('./test_input.csv', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f67aed74-9059-48a0-b0b8-3750f8ec22fe",
      "metadata": {
        "ExecutionIndicator": {
          "show": true
        },
        "scrolled": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f67aed74-9059-48a0-b0b8-3750f8ec22fe",
        "outputId": "fd4ff79c-2be3-4cbe-bf69-50544668f9cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "æ•°æ®é›†çš„å¤§å°ä¸º: 2972\n",
            "å‰50æ¡æ•°æ®å¦‚ä¸‹ï¼š\n",
            "\n",
            "åŒæ–¹å¿ƒå¾€ä¸€å¤„æƒ³ï¼Œæ„å¿—åšå®šã€‚\n",
            "ä¸¤ç«¯éƒ½æä¸ºè‰°éš¾ï¼Œéš¾ä»¥ä½œå‡ºå†³å®šã€‚\n",
            "é›„æµ‘æ·±è¿œçš„æ„æ—¨ã€‚ç»†è…»å¾®å¦™ã€‚é«˜è¶…å·§å¦™ã€‚\n",
            "æè¿°æ°´æµæ¹æ€¥ï¼Œä¸”å¿«é€Ÿä¸”æ·±è¿œã€‚\n",
            "é¿å…è¢«å¼•è¯±åšå‡ºä¸é“å¾·æˆ–å¯ç–‘çš„è¡Œä¸ºã€‚\n",
            "è‚˜éƒ¨è‚©çªã€‚æ¯”å–»äº‹ç‰©å‘ç”Ÿäºèº«è¾¹ã€‚\n",
            "ä»–å¼ å¼€å˜´å·´ï¼Œåå’½ç€å£æ°´ã€‚\n",
            "ä»…è§ä¸€é¢ï¼Œä¸è¶³ä»¥æ·±å…¥äº†è§£ã€‚\n",
            "å‘å››å¤„æ•£å‘æ–½èˆï¼ŒåŒæ—¶æ‰‹ä¸­æ‹¿ç€ç¢—ç›†ã€‚\n",
            "æ¯”å–»æŠŠè£…å¤‡è„±ä¸‹ï¼Œæ”¾ä¸‹æ­¦å™¨ã€‚\n",
            "å¯¹äºä»»ä½•æˆ˜å½¹ï¼Œéƒ½è¦ä¸€è´¥æ¶‚åœ°ã€‚\n",
            "é˜´è°‹çœŸç›¸å·²ç»æ˜­ç„¶äºä¼—ã€‚\n",
            "æ¯”å–»å› ä¸ºæ— èƒ½ä¸ºåŠ›è€Œå†™ä¸‹äº†å¥½æ–‡ç« ã€‚\n",
            "å¤ä»£æ–‡äººå£«å¤§å¤«ç»å¸¸ä¸¾è¡Œè¯—æ­Œæœ—è¯µä¼šã€‚\n",
            "ä¸èƒ½å½¢å®¹ä¸ºä¸é«˜å…´ï¼Œåªèƒ½è¯´æ˜æ²¡åŠ²å„¿ã€‚\n",
            "è¡¨ç°å¾—ä¸¾æ­¢ç«¯åº„ï¼Œå¾ˆæœ‰æ•™å…»ã€‚\n",
            "1.æ´—åˆ·å…µå™¨2.å–‚å…»æˆ˜é©¬3.å‡†å¤‡ä½œæˆ˜\n",
            "å…³æ³¨ç”Ÿå‘½å‚å±è€…ï¼Œå…³æ€€æ¿’å±è€…ã€‚\n",
            "æ¯”å–»é¢å¯¹æŒ‘æˆ˜ï¼ŒåšéŸ§ä¸æ‹”åœ°å‰è¡Œã€‚\n",
            "è¿‡å»çš„ç§‘ä¸¾è€ƒè¯•ä¸­è¢«é€‰æ‹”ä¸ºè¿›å£«çš„ç§°å·ã€‚\n",
            "ç›¸ä¼¼ç¨‹åº¦æé«˜æˆ–ç›¸å·®æ— å‡ ã€‚\n",
            "æ¯”å–»ä¸æ–­åœ°è¡¥å……ã€å †ç Œå’Œå»¶ä¼¸ã€‚\n",
            "ä»¥å®‰é€¸å¿«ä¹çš„ç”Ÿæ´»å’ŒåŠ³åŠ¨ä¸ºé‡ã€‚\n",
            "å¥¹ä»ç„¶æ¯å¤©æ•™å¯¼å¥¹çš„å„¿å­ã€‚\n",
            "çŠ¹ä»¥ç«ä¸ºè€•ï¼Œæ¯”å–»åŸå§‹ã€ç®€æœ´çš„å†œè€•æ–¹å¼ã€‚\n",
            "æŒ‡å›°å¢ƒä¸­å¤„äºä¸åˆ©åœ°ä½ã€‚\n",
            "æœé›†å’Œç ”ç©¶å…¶å†…åœ¨é“ç†ã€‚\n",
            "æ²¡æœ‰ä»»ä½•äººå¸®åŠ©å’Œæ”¯æŒã€‚\n",
            "è¿™ä½ä½œè€…çš„æ–‡ç« é£æ ¼ä¸è‡ªå·±éå¸¸ç›¸ä¼¼ã€‚\n",
            "å›½åŠ›å¼ºå¤§ï¼Œå†›äº‹åŠ›é‡å·²ç»åœæ­¢ã€‚\n",
            "ç›¸äº’å‹¾ç»“ç»´æŒï¼›ç›¸äº’åˆ©ç”¨ã€‚\n",
            "æ¯”å–»è¿›ç¨‹é£å¿«ï¼Œæ—¥è¡Œåƒé‡Œã€‚\n",
            "åªæœ‰ä¸€ä¸ªç›®çš„ï¼Œè¿½æ±‚åˆ©æ¶¦ã€‚\n",
            "åŠ¡å¿…è°¨æ…å¯¹å¾…ï¼Œæ…é‡å¤„ç†äº‹åŠ¡ã€‚\n",
            "æ— æ³•è¨€è¡¨ï¼Œåªèƒ½æ„Ÿæ…¨ä¸‡åƒã€‚\n",
            "å½¢å®¹æ°”åŠ¿ç£…ç¤´çš„æ–‡ç« é£æ ¼ã€‚\n",
            "æ ¹æ®è´¡çŒ®çš„å¤§å°ç»™äºˆå¥–åŠ±ã€‚\n",
            "è®©å›½å®¶è’™ç¾ï¼Œæ°‘ä¼—è’™éš¾ã€‚\n",
            "å½¢å®¹æœ‰æƒåŠ¿çš„äººæå…¶æ®‹å¿å’Œæ— ç¤¼ã€‚\n",
            "åšå†³è¦æ±‚å†æ¬¡å‘æŸäººå¼ºè°ƒã€‚\n",
            "é‡‡å–æªæ–½ï¼›é‡‡å–åŠæ³•ï¼›é‡‡å–è¡ŒåŠ¨ï¼›å®è¡Œ\n",
            "å¿ƒä¸­å……æ»¡äº†ç–‘æƒ‘ï¼Œè¿˜æ²¡æœ‰æ‰¾åˆ°è§£ç­”ã€‚\n",
            "ç´¯ç´¯ç½ªè¡Œï¼Œéå†æ— ç©·ã€‚å½¢å®¹ç½ªæ¶æé‡ã€‚\n",
            "å½¢å®¹äººçš„å®¹è²Œæ¸…çˆ½ä¿Šé›…ï¼Œé£åº¦ç¿©ç¿©ã€‚\n",
            "è¾æ‰æœ¬èŒå·¥ä½œå»åšå…¶ä»–çš„äº‹æƒ…ã€‚\n",
            "ç§¦æ±‰æ—¶æœŸï¼Œå‹‹ä½è‡³é«˜è€…éƒ½ä½©æˆ´é‡‘å°å’Œç´«ç»¶ã€‚\n",
            "åˆ›ç«‹ç‹¬ç‰¹çš„é£æ ¼ï¼Œä¸ä¼—ä¸åŒã€‚\n",
            "åˆ°å¤„éƒ½æ˜¯å†°é›ªè¦†ç›–çš„ç¯å¢ƒï¼Œå½¢å®¹ä¸¥å†¬å¤©æ°”ã€‚\n",
            "æ—§äº‹ç‰©è¢«åºŸå¼ƒï¼›ä¸ºäº†æ–°äº‹ç‰©è€Œé‡‡å–æªæ–½ã€‚\n",
            "â‘ å‘äººè¡Œç¤¼ã€‚â‘¡ç”¨ä½œå“€æ‚¼è¯æˆ–ç¥­å¥ è¯­ã€‚\n"
          ]
        }
      ],
      "source": [
        "# æŸ¥çœ‹æ•°æ®é›†å¤§å°\n",
        "print(f\"æ•°æ®é›†çš„å¤§å°ä¸º: {test.shape[0]}\\nå‰50æ¡æ•°æ®å¦‚ä¸‹ï¼š\\n\")\n",
        "\n",
        "# æŸ¥çœ‹å‰50æ¡èµ›äº‹æ•°æ®é›†ï¼ˆèµ›é¢˜è¦æ±‚æ ¹æ®æ¯è¡Œå¥å­ï¼Œç»™å‡º5ä¸ªå¯èƒ½åŒ¹é…çš„æˆè¯­ï¼‰\n",
        "for test_prompt in test[0].values[:50]:\n",
        "    print(test_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a5fbc56-26d4-4131-b8f9-452899ead64b",
      "metadata": {
        "id": "1a5fbc56-26d4-4131-b8f9-452899ead64b"
      },
      "source": [
        "## æ­¥éª¤5ï¼šè¾“å‡ºæˆè¯­"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "çˆ†æ˜¾å­˜(out of memoryï¼‰æ¯”è¾ƒä¸¥é‡ï¼Œç›®å‰Colab A100å•å¼ 40GBæ˜¾å­˜è¿˜ä¸å¤Ÿç”¨ã€‚æœŸå¾…å¤§ä½¬çš„æ„è§ã€‚"
      ],
      "metadata": {
        "id": "VAf99SvFRkwh"
      },
      "id": "VAf99SvFRkwh"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "154ae267-49b5-4250-b0ad-c89b5c7ff3da",
      "metadata": {
        "ExecutionIndicator": {
          "show": true
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "154ae267-49b5-4250-b0ad-c89b5c7ff3da",
        "outputId": "ac9043a5-7f28-4233-f5f1-198599ab6bf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rå¤„ç†è¿›åº¦:   0%|          | 0/2972 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 10-29 03:15:41 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
            "INFO 10-29 03:15:41 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='THUDM/glm-4-9b-chat', speculative_config=None, tokenizer='THUDM/glm-4-9b-chat', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=THUDM/glm-4-9b-chat, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n",
            "WARNING 10-29 03:15:42 tokenizer.py:169] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
            "INFO 10-29 03:15:42 model_runner.py:1056] Starting to load model THUDM/glm-4-9b-chat...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rå¤„ç†è¿›åº¦:   0%|          | 0/2972 [00:02<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 39.56 GiB of which 26.81 MiB is free. Process 105528 has 39.52 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 60.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-d0b24688168a>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹(LLM)ï¼Œé…ç½®æ¨¡å‹å¹¶è¡Œå¤§å°ã€æœ€å¤§æ¨¡å‹é•¿åº¦ç­‰å‚æ•°\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     llm = LLM(\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtensor_parallel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtp_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, mm_processor_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         )\n\u001b[0;32m--> 177\u001b[0;31m         self.llm_engine = LLMEngine.from_engine_args(\n\u001b[0m\u001b[1;32m    178\u001b[0m             engine_args, usage_context=UsageContext.LLM_CLASS)\n\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36mfrom_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0mexecutor_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_executor_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;31m# Create the LLM engine.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m         engine = cls(\n\u001b[0m\u001b[1;32m    574\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mengine_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0mexecutor_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecutor_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, speculative_config, decoding_config, observability_config, prompt_adapter_config, executor_class, log_stats, usage_context, stat_loggers, input_registry, use_cached_outputs)\u001b[0m\n\u001b[1;32m    332\u001b[0m             model_config)\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         self.model_executor = executor_class(\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0mmodel_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mcache_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/executor/executor_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, speculative_config, prompt_adapter_config, observability_config)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_adapter_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_adapter_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservability_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservability_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_executor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\u001b[0m in \u001b[0;36m_init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver_worker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver_worker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver_worker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     def _get_worker_kwargs(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     def save_sharded_state(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1056\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting to load model %s...\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mDeviceMemoryProfiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m             self.model = get_model(model_config=self.model_config,\n\u001b[0m\u001b[1;32m   1059\u001b[0m                                    \u001b[0mdevice_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m                                    \u001b[0mload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/__init__.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(model_config, load_config, device_config, parallel_config, scheduler_config, lora_config, cache_config)\u001b[0m\n\u001b[1;32m     17\u001b[0m               cache_config: CacheConfig) -> nn.Module:\n\u001b[1;32m     18\u001b[0m     \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     return loader.load_model(model_config=model_config,\n\u001b[0m\u001b[1;32m     20\u001b[0m                              \u001b[0mdevice_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                              \u001b[0mlora_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlora_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, model_config, device_config, lora_config, parallel_config, scheduler_config, cache_config)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mset_default_torch_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtarget_device\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m                 model = _initialize_model(model_config, self.load_config,\n\u001b[0m\u001b[1;32m    399\u001b[0m                                           \u001b[0mlora_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m                                           scheduler_config)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\u001b[0m in \u001b[0;36m_initialize_model\u001b[0;34m(model_config, load_config, lora_config, cache_config, scheduler_config)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_architecture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     return build_model(\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhf_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(model_class, hf_config, cache_config, quant_config, lora_config, multimodal_config, scheduler_config)\u001b[0m\n\u001b[1;32m    158\u001b[0m                                                     scheduler_config)\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     return model_class(config=hf_config,\n\u001b[0m\u001b[1;32m    161\u001b[0m                        \u001b[0mcache_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                        \u001b[0mquant_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquant_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/chatglm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, multimodal_config, cache_config, quant_config, lora_config)\u001b[0m\n\u001b[1;32m    595\u001b[0m         self.max_position_embeddings = getattr(config, \"max_sequence_length\",\n\u001b[1;32m    596\u001b[0m                                                8192)\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatGLMModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtie_word_embeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m             self.transformer.output_layer.weight = (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/chatglm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, cache_config, quant_config)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m         self.embedding = VocabParallelEmbedding(config.padded_vocab_size,\n\u001b[0m\u001b[1;32m    488\u001b[0m                                                 \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                                                 quant_config=quant_config)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/vocab_parallel_embedding.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_embeddings, embedding_dim, params_dtype, org_num_embeddings, padding_size, quant_config, prefix)\u001b[0m\n\u001b[1;32m    258\u001b[0m             self.shard_indices.added_vocab_start_index)\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         self.linear_method.create_weights(self,\n\u001b[0m\u001b[1;32m    261\u001b[0m                                           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                                           \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_embeddings_per_partition\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/vocab_parallel_embedding.py\u001b[0m in \u001b[0;36mcreate_weights\u001b[0;34m(self, layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype, **extra_weight_attrs)\u001b[0m\n\u001b[1;32m     26\u001b[0m                        **extra_weight_attrs):\n\u001b[1;32m     27\u001b[0m         \u001b[0;34m\"\"\"Create weights for embedding layer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         weight = Parameter(torch.empty(sum(output_partition_sizes),\n\u001b[0m\u001b[1;32m     29\u001b[0m                                        \u001b[0minput_size_per_partition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                                        dtype=params_dtype),\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_device_constructors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# NB: This is directly called from C++ in torch/csrc/Device.cpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.16 GiB. GPU 0 has a total capacity of 39.56 GiB of which 26.81 MiB is free. Process 105528 has 39.52 GiB memory in use. Of the allocated memory 38.96 GiB is allocated by PyTorch, and 60.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "\n",
        "i = 1\n",
        "# å‡è®¾ test æ˜¯ä¸€ä¸ª DataFrame\n",
        "# éå†æµ‹è¯•æ•°æ®é›†çš„ç¬¬ä¸€é¡¹çš„å€¼ï¼Œç›®çš„æ˜¯ç”Ÿæˆä¸ç»™å®šå¥å­æœ€ç›¸å…³çš„äº”ä¸ªæˆè¯­\n",
        "for test_prompt in tqdm(test[0].values, total=len(test[0].values), desc=\"å¤„ç†è¿›åº¦\"):\n",
        "    i = i + 1\n",
        "    # æ„é€ æç¤ºä¿¡æ¯ï¼Œè¦æ±‚æ¨¡å‹è¾“å‡ºä¸å¥å­æœ€ç›¸å…³çš„äº”ä¸ªæˆè¯­\n",
        "    prompt = [{\"role\": \"user\", \"content\": f\"åˆ—ä¸¾ä¸ä¸‹é¢å¥å­æœ€ç¬¦åˆçš„äº”ä¸ªæˆè¯­ã€‚åªéœ€è¦è¾“å‡ºäº”ä¸ªæˆè¯­ï¼Œä¸éœ€è¦æœ‰å…¶ä»–çš„è¾“å‡ºï¼Œå†™åœ¨ä¸€è¡Œä¸­ï¼š{test_prompt}\"}]\n",
        "\n",
        "    # åˆå§‹åŒ–ä¸€ä¸ªé•¿åº¦ä¸º5çš„åˆ—è¡¨ï¼Œå¡«å……é»˜è®¤æˆè¯­â€œåŒèˆŸå…±æµâ€\n",
        "    words = ['åŒèˆŸå…±æµ'] * 5\n",
        "\n",
        "    # ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹åˆå§‹åŒ–tokenizerï¼Œä¿¡ä»»è¿œç¨‹ä»£ç ä»¥æ”¯æŒå¯èƒ½çš„è‡ªå®šä¹‰å®ç°\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "    # åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹(LLM)ï¼Œé…ç½®æ¨¡å‹å¹¶è¡Œå¤§å°ã€æœ€å¤§æ¨¡å‹é•¿åº¦ç­‰å‚æ•°\n",
        "    llm = LLM(\n",
        "    model=model_name,\n",
        "    tensor_parallel_size=tp_size,\n",
        "    max_model_len=max_model_len,\n",
        "    trust_remote_code=True,\n",
        "    enforce_eager=True,\n",
        "    gpu_memory_utilization=0.8\n",
        "\n",
        "    )\n",
        "\n",
        "    # å®šä¹‰åœæ­¢ç”Ÿæˆçš„token IDåˆ—è¡¨ï¼Œç”¨äºæ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„ç»“æŸ\n",
        "    stop_token_ids = [151329, 151336, 151338]\n",
        "    # è®¾ç½®é‡‡æ ·å‚æ•°ï¼ŒåŒ…æ‹¬æ¸©åº¦ã€æœ€å¤§ç”Ÿæˆtokenæ•°å’Œåœæ­¢token ID\n",
        "    sampling_params = SamplingParams(temperature=0.95, max_tokens=1024, stop_token_ids=stop_token_ids)\n",
        "    # ä½¿ç”¨tokenizerå°†æç¤ºè½¬åŒ–ä¸ºæ¨¡å‹æ‰€éœ€çš„è¾“å…¥æ ¼å¼ï¼Œä¸è¿›è¡ŒtokenåŒ–ï¼Œæ·»åŠ ç”Ÿæˆæç¤º\n",
        "    inputs = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
        "    # ç”Ÿæˆæ–‡æœ¬è¾“å‡ºï¼Œæ ¹æ®è¾“å…¥æç¤ºå’Œé‡‡æ ·å‚æ•°è¿›è¡Œæ–‡æœ¬ç”Ÿæˆ\n",
        "    outputs = llm.generate(prompts=inputs, sampling_params=sampling_params)\n",
        "\n",
        "    # è§£ç æ¨¡å‹è¾“å‡ºï¼Œå»é™¤ç‰¹æ®Šæ ‡è®°\n",
        "    response = outputs[0].outputs[0].text\n",
        "\n",
        "    # æ¸…ç†å›ç­”æ–‡æœ¬ï¼Œç¡®ä¿æ ¼å¼ç»Ÿä¸€ éœ€è¦å»æ‰çš„å†…å®¹åŒ…æ‹¬å¸¸è§„æ ‡ç‚¹ç¬¦å·ï¼Œå…¶ä¸­'\\n'æ˜¯æ¢è¡Œç¬¦ï¼Œä½¿è¾“å‡ºå›åˆ°åŒä¸€è¡Œå†…\n",
        "    response = response.replace('\\n', ' ').replace('ã€', ' ').replace(',', ' ').replace('ï¼›',' ').replace('ã€‚',' ')\n",
        "    # æå–å›ç­”ä¸­çš„æˆè¯­ï¼Œç¡®ä¿æ¯ä¸ªæˆè¯­é•¿åº¦ä¸º4ä¸”éç©º\n",
        "    words = [x for x in response.split() if len(x) == 4 and x.strip() != '']\n",
        "\n",
        "\n",
        "    # å¦‚æœç”Ÿæˆçš„æˆè¯­åˆ—è¡¨é•¿åº¦ä¸æ»¡è¶³è¦æ±‚ï¼ˆå³20ä¸ªå­—ç¬¦ï¼‰ï¼Œåˆ™ä½¿ç”¨é»˜è®¤æˆè¯­åˆ—è¡¨\n",
        "   #if len(' '.join(words).strip()) != 24:\n",
        "       # words = ['åŒèˆŸå…±æµ'] * 5\n",
        "    while True:\n",
        "        text = ' '.join(words).strip()\n",
        "        if len(text) < 24:\n",
        "            words.append('åŒèˆŸå…±æµ')\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # å°†æœ€ç»ˆçš„æˆè¯­åˆ—è¡¨å†™å…¥æäº¤æ–‡ä»¶\n",
        "    with open('submit.csv', 'a+', encoding='utf-8') as up:\n",
        "        up.write(' '.join(words) + '\\n')\n",
        "\n",
        "\n",
        "    # æŸ¥çœ‹é˜¶æ®µæ€§ç»“æœ\n",
        "    if i % 50 == 0:\n",
        "        tqdm.write(f\"å¤§æ¨¡å‹ç¬¬{i}æ¬¡è¿”å›çš„ç»“æœæ˜¯ï¼š\\n   {response}\\n\")\n",
        "        tqdm.write(f\"submit.cvsç¬¬{i}è¡Œè¾“å‡ºç»“æœï¼š\\n   {words}\\n\")\n",
        "\n",
        "    # ä¸ºäº†å°½å¿«æ‹¿åˆ°ç»“æœï¼Œæˆ‘ä»¬æš‚æ—¶ä»…è·å¾—500ä¸ªç»“æœï¼ˆå¦‚æœæœ‰æ—¶é—´çš„è¯ï¼Œå¯ä»¥åˆ é™¤è¿™ä¸¤è¡Œï¼‰\n",
        "    if i == 2973:\n",
        "        break\n",
        "\n",
        "print('submit.csv å·²ç”Ÿæˆ')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "891607d15775446ca8f6c518bf335673": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_723a52242bac430b97a607b186dddc74",
              "IPY_MODEL_ce19752078d6444dbe80f73045477346",
              "IPY_MODEL_e83363cb2184413590238f99f62667d3"
            ],
            "layout": "IPY_MODEL_075ae5356a1d4358a0b86205b15d4f49"
          }
        },
        "723a52242bac430b97a607b186dddc74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3dd29182748439ea862b822b6d81a49",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1b3b1720befe41fa90d423d7dd4c5731",
            "value": ""
          }
        },
        "ce19752078d6444dbe80f73045477346": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cad687a7b5084c8ca7e2300090e93e7c",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2465db233c024598a621a68e0e986222",
            "value": 10
          }
        },
        "e83363cb2184413590238f99f62667d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34393296c08c40e2b0eb3c7888af9136",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_add46b1cf2724f7e9aec0ffe280f72a8",
            "value": "Loadingâ€‡safetensorsâ€‡checkpointâ€‡shards:â€‡100%â€‡Completedâ€‡|â€‡10/10â€‡[00:05&lt;00:00,â€‡â€‡1.84it/s]\n"
          }
        },
        "075ae5356a1d4358a0b86205b15d4f49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3dd29182748439ea862b822b6d81a49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b3b1720befe41fa90d423d7dd4c5731": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cad687a7b5084c8ca7e2300090e93e7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2465db233c024598a621a68e0e986222": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "34393296c08c40e2b0eb3c7888af9136": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "add46b1cf2724f7e9aec0ffe280f72a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}