第一课笔记：
【MindSpore:大模型时代的高性能易用框架】
1大规模语言模型的发展

大模型往往是指具有巨大的参数量、训练数据量、算力需求并且是Transformer框架的模型
一般称7B参数以上的模型为大模型，ScalingLaw论文中更是称68B以上的模型为大模型
大模型发展仍在朝着更大的参数量（千亿到万亿）、结构稠密到稀疏、冲击着硬件算力的上限。
现有的Transformer结构仍然没有达到上限，基本上集中到Decoder方向进行无监督下学习。如今的预训练环节和微调环节成本虽经过一轮下降，但仍是比较难以为科研课题组承担，所以基本上是在企业端大厂在做这个方向。高校能承担的成本内，一般是让学生做高效参数微调（PEFT）
2MindSpore技术架构
技术框架的位置与PyTorch比较接近，上承模型算法工程，下接芯片算力使能。其包含了MindFormers、MindOne、MindRLHF、MindPET等套件，主要能力是数据预处理、开发接口、调试调优、编译执行、推理部署。总的来说，是用来开发大模型的工具框架。
对于科研、科学计算来说，这是个具有高性能和更好效能比的一个工具。对于企业端，这个是进行并行计算训练超大规模模型的计算框架。性能上，提升集群的算力利用率，降低故障率；成本上，降低迁移部署周期；开发上，提速开发和调试调优。
MindSpore技术框架上承应用使能：ModelArts、HIAI Science、MindX；下接CANN。
技术架构:包含ModelZoo\大模型套件、领域套件、科学计算套件、MindTorch等，MindExpression、MindChute、MindData、MindCompiler、MindInsight、MindArmour、MindIR（用于端边云无缝迁移）、MindRT等。
支持多维混合并行、集群高可用、动静态图统一、开发套件丰富、API统一
Mindformers商用套件进行极致性能优化
3MindSpore分布式并行优势
大集群训练大模型的挑战
内存墙：200B的模型要占用745GB内存，训练需要128张卡。 多维混合并行
性能墙：大模型切分到集群后，通信成了主要的性能瓶颈。 多维内存优化
效率墙：算法的分布式并行开发一直是个难题。 手工并行-半自动并行-自动并行
调优墙：数千节点的集群上，很难保证计算的正确性 可视化集群调试调优
挑战对应的优势
多维混合并行：自动进行AI编译、选择合适的并行策略自动执行，降低了开发门槛。并行策略：数据并行、算子级并行、流水线并行、MoE并行、多副本并行、数据切片并行、优化器并行、子图并行。
多维存储及异构优化：全局内存复用、重计算、CPU offload、NVMe Offload、执行库优化、MemSwap。
MindSpore的毛线哦ing计算流程和存储开销的高效优化方法：动态数据优化
MindSpore分布式并行编程范式：手动并行（集合通信）、半自动并行（算子级并行、流水线并行等配置一下参数就行）、自动并行
4MindSpore2.x的易用性提升
综合了面向对象编程（例如PyTorch)和函数式编程（例如做AI科学计算的JAX)的写法。
为了进行整图计算和易用性的兼顾而设计的混合编程方式写法。
提供了MindTorch(所有接口与PyTorch一样，但是变慢了）和MindNLP两种套件。
【MindNLP：基于MindSpore最好用的NLP/LLM开发框架】
为什么需要NLP套件？
自然语言处理的大模型集中在transformers的Decoder-Only框架,这个框架下需要对于预训练、微调和RLHF的训练需求。
另外，高校有对PEFT的需求。（毕竟高校没有那么多的资金供学生进行预训练和全量微调）
ChatLLM和RAG的推理应用需求
基于MindSpore优势特性的NLP套件设计
NLP套件需要：开发简便、高性能训练推理、简单高效的数据预处理、快速使用预训练模型。
MindSpore的优势特性：混合式编程写法、动静态图统一、数据处理引擎、自动并行策略
MindSpore Dataset可以高效加载数据处理流水线任务。
MindNLP特性：全平台支持昇腾算力、CPU、GPU；大量模型；支持主流微调方法；分布式并行推理；量化算法支持；SentenceTransformer支持；动态图性能优化；同静态图统一；海量LLM应用持续更新。
嫁接HuggingFace生态
顺应科研和开发寻求，嫁接Huggingface的托管服务（国内镜像）、核心框架和训练框架
MindSpore数据引擎和Datasets解决问题：数据格式不统一、离线处理需要额外的实现、完整数据集加载消耗大量显存、数据清晰工作量大
Datasets区别:与Pytorch全量数据下载到显存进行加载的方法相比，MindNLP只下载设定量的数据分批进显存处理。但是接口封装一致，不会改多少代码。
Tokenizer区别：直接统一接口使用，又快又方便。
能直接在MindSpore里使用Huggingface模型，支持6大镜像站简单解决模型使用问题。
PEFT微调
保证了Huggingface用例的情况下，精度与Pytorch一致，在昇腾上面用能高个0.5到1个点的表现提升。
大模型推理
动态图性能优化，能够帮助部署工程师实现性能提升。
同静态图切换、分布式推同静态图切换、分布式推理、分布式训练Megatron
社区贡献指南（Huggingface模型迁移）